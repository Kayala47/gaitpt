{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Define-a-pytorch-Dataset-object-to-contain-the-training-and-testing-data\" data-toc-modified-id=\"Define-a-pytorch-Dataset-object-to-contain-the-training-and-testing-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Define a pytorch Dataset object to contain the training and testing data</a></span></li><li><span><a href=\"#Define-training-methods-for-the-model\" data-toc-modified-id=\"Define-training-methods-for-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Define training methods for the model</a></span></li><li><span><a href=\"#Define-testing-methods-for-the-model\" data-toc-modified-id=\"Define-testing-methods-for-the-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Define testing methods for the model</a></span></li><li><span><a href=\"#Define-plotting-method-for-loss\" data-toc-modified-id=\"Define-plotting-method-for-loss-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Define plotting method for loss</a></span></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define Model Architecture</a></span></li><li><span><a href=\"#Define-Run-function\" data-toc-modified-id=\"Define-Run-function-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Define Run function</a></span></li><li><span><a href=\"#Create-Datasets-for-Each-Gait-File\" data-toc-modified-id=\"Create-Datasets-for-Each-Gait-File-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Create Datasets for Each Gait File</a></span></li><li><span><a href=\"#Run-and-plot-results\" data-toc-modified-id=\"Run-and-plot-results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Run and plot results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "from math import sqrt, inf\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# print(\"USING pytorch VERSION: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants and Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANIMATIONS_PATH = Path(\"Animations/\")\n",
    "DATA_PATH = Path(\"Data/\")\n",
    "SANITY_PATH = Path(\"Sanity_Checks/\")\n",
    "MODEL_OUTPUT_PATH = Path(\"Model_Outputs/\")\n",
    "MODEL_PATH = Path(\"Models/\")\n",
    "TMP_PATH = Path(\"TMP/\")\n",
    "CSV_HEADER = [\n",
    "                \"FL A1 DF 1\",\n",
    "                \"FL A1 DF 2\",\n",
    "                \"FL A2 DF 1\",\n",
    "                \"FL A2 DF 2\",\n",
    "                \"FL A3 DF 1\",\n",
    "                \"FL A3 DF 2\",\n",
    "                \"FR A1 DF 1\",\n",
    "                \"FR A1 DF 2\",\n",
    "                \"FR A2 DF 1\",\n",
    "                \"FR A2 DF 2\",\n",
    "                \"FR A3 DF 1\",\n",
    "                \"FR A3 DF 2\",\n",
    "                \"BL A1 DF 1\",\n",
    "                \"BL A1 DF 2\",\n",
    "                \"BL A2 DF 1\",\n",
    "                \"BL A2 DF 2\",\n",
    "                \"BL A3 DF 1\",\n",
    "                \"BL A3 DF 2\",\n",
    "                \"BR A1 DF 1\",\n",
    "                \"BR A1 DF 2\",\n",
    "                \"BR A2 DF 1\",\n",
    "                \"BR A2 DF 2\",\n",
    "                \"BR A3 DF 1\",\n",
    "                \"BR A3 DF 2\",\n",
    "                \"SP A1 DF 1\",\n",
    "                \"SP A1 DF 2\",\n",
    "                \"SP A2 DF 1\",\n",
    "                \"SP A2 DF 2\",\n",
    "            ]\n",
    "\n",
    "AVGS_KEY = \"canter\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our data is in numpy format, which conveniently pytorch has a method for converting to their native format.\n",
    "\n",
    "The init function takes the path to the csv and creates a dataset out of it. I actually have three different options here. The dataset could be composed such that x is the 'timestamp' of the movement,the previous set of angles, or a tuple of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor  # for MSE or L1 Loss\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def create_datasets(csv_path: str, train_perc: float = 0.8, nosplit=False):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    length = len(df)\n",
    "    time = 10\n",
    "    timestep = 0.005\n",
    "\n",
    "    # x_data = np.array([])\n",
    "    # y_data = np.array([])\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    sin_test_timepoints = (\n",
    "        np.random.rand(length, 1) * time\n",
    "    )  # Repeat data generation for test set\n",
    "    sin_test_timepoints = sin_test_timepoints.ravel()\n",
    "    sin_iter = iter(sin_test_timepoints)\n",
    "\n",
    "    # data order = sin, angles, torso, touch_sens\n",
    "\n",
    "    # if x = curr angles and y = next angles\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        if i < length - 1:\n",
    "            x = np.append(x, df.iloc[i])\n",
    "            y = np.append(y, df.iloc[i + 1][:-4])  # only include angles\n",
    "        else:\n",
    "            # since it loops anyway\n",
    "            x = np.append(x, df.iloc[i])\n",
    "            y = np.append(y, df.iloc[0][:-4])\n",
    "\n",
    "        x = np.append([next(sin_iter)], x)\n",
    "\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "\n",
    "    x_data = np.array(x_data, dtype=np.float64)\n",
    "    y_data = np.array(y_data, dtype=np.float64)\n",
    "\n",
    "    if not nosplit:\n",
    "\n",
    "        last_train_idx = int(len(x_data) * train_perc)\n",
    "\n",
    "        train_x = x_data[:last_train_idx]\n",
    "        train_y = y_data[:last_train_idx]\n",
    "        test_x = x_data[last_train_idx:]\n",
    "        test_y = y_data[last_train_idx:]\n",
    "\n",
    "        return AngleDataset(x=train_x, y=train_y), AngleDataset(x=test_x, y=test_y)\n",
    "    else:\n",
    "        return AngleDataset(x=x_data, y=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['canter', 'gallop', 'trot', 'walk'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get averages for each csv\n",
    "\n",
    "def get_avg_range(df_col):\n",
    "    return (df_col.min() + df_col.max()) / 2\n",
    "\n",
    "def get_avgs(fp: str):\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    return df.apply(get_avg_range, axis=0).tolist() #0 axis is cols, 1 is rows\n",
    "\n",
    "avgs = {}\n",
    "\n",
    "\n",
    "for file in DATA_PATH.glob(\"*_kinematic.csv\"):\n",
    "    avgs[file.stem.split(\"_\")[0]] = get_avgs(file)\n",
    "\n",
    "avgs.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training methods for the model\n",
    "These methods use an initialized model and training data to iteratively perform the forward and backward pass of optimization. Aside from some data reformatting that depends on the input, output, and loss function, these methods will always be the same for any shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, x, y, optimizer, loss_fn):\n",
    "    # Run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn, config):\n",
    "    losses = list()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_index = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        loss = train_batch(\n",
    "            model=model, x=x, y=y, optimizer=optimizer, loss_fn=loss_fn\n",
    "        )\n",
    "        \n",
    "        losses.append(loss)            \n",
    "        batch_index += 1\n",
    "\n",
    "    # if e % 50 == 0:\n",
    "        # pass\n",
    "        #   ic(\"Epoch: \", e+1)\n",
    "        #   ic(\"Batches: \", batch_index)\n",
    "\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define testing methods for the model\n",
    "These methods are like training, but we don't need to update the parameters of the model anymore because when we call the test() method, the model has already been trained. Instead, this method just calculates the predicted y values and returns them, AKA the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, x, y):\n",
    "    # run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    correct = (y_predict == y).sum().item()\n",
    "    total = y.size(0)\n",
    "\n",
    "    return y, y_predict, correct, total\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    y_vectors = list()\n",
    "    y_predict_vectors = list()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_index = 0\n",
    "    for x, y in loader:\n",
    "        y, y_predict, batch_correct, batch_total = test_batch(model=model, x=x, y=y)\n",
    "\n",
    "        y_vectors.append(y.data.numpy())\n",
    "        y_predict_vectors.append(y_predict.data.numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "    y_predict_vector = np.concatenate(y_predict_vectors)\n",
    "\n",
    "    return y_predict_vector, (correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting method for loss\n",
    "This is a plotting method for looking at the behavior of the loss over training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, title: str, show=True):\n",
    "    fig = pyplot.gcf()\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax = pyplot.axes()\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    x_loss = list(range(len(losses)))\n",
    "    pyplot.title(title)\n",
    "\n",
    "    pyplot.plot(x_loss, losses)\n",
    "\n",
    "    if show:\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture\n",
    "- 33 inputs = 3 joint angles per leg, 4 legs, 2 DOF per joint. 4 touch sensors. 1 sine timestamp.\n",
    "- 28 outputs = *same as above, except just the joint angles*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitModel(nn.Module):\n",
    "    def __init__(self, layer_sizes, config):\n",
    "        super(GaitModel, self).__init__()\n",
    "        self.avgs = torch.FloatTensor(avgs[AVGS_KEY])\n",
    "        hidden_layers = []\n",
    "\n",
    "      \n",
    "        for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes):\n",
    "            layers = [nn.Linear(nlminus1, nl), nn.ReLU()]\n",
    "            if config[\"batch_norm\"]:\n",
    "                layers.append(nn.BatchNorm1d(nl))\n",
    "            \n",
    "            # if config[\"dropout_rate\"] > 0.0:\n",
    "            layers.append(nn.Dropout(config[\"dropout_rate\"]))\n",
    "            \n",
    "\n",
    "            hidden_layers.append(nn.Sequential(*layers))\n",
    "\n",
    "        # random comment for git testing\n",
    "\n",
    "        # The output layer does not include an activation function.\n",
    "        # See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # tanh = torch.nn.Tanh()\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        tmp = np.zeros(X.shape)\n",
    "        tmp[:, 0] = X[:, 0] #first\n",
    "        X2 = X[:, 1:]\n",
    "        X2 -= self.avgs\n",
    "\n",
    "        tmp[:, 1:] = X2[:, :]\n",
    "        X = torch.FloatTensor(tmp)\n",
    "\n",
    "        X = self.layers(X)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp = np.zeros(X.shape)\n",
    "            X += self.avgs[4:]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets for Each Gait File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('canter', {'lr': 0.07562329966776266, 'momentum': 0.2740067632362447, 'batch_norm': True, 'dropout_rate': 0.59104418584669, 'epochs': 240, 'hidden_layers': 1, 'min_layers': 2, 'max_layers': 33}, <__main__.AngleDataset object at 0x000001F8DEB61AC8>), ('gallop', {'lr': 0.0933321557194551, 'momentum': 0.3745454755588069, 'batch_norm': True, 'dropout_rate': 0.21707887130206255, 'epochs': 898, 'hidden_layers': 3, 'min_layers': 2, 'max_layers': 50}, <__main__.AngleDataset object at 0x000001F8DEC08208>), ('trot', {'lr': 0.10744097713272023, 'momentum': 0.8101267432516481, 'batch_norm': False, 'dropout_rate': 0.09524235885285767, 'epochs': 649, 'hidden_layers': 10, 'min_layers': 2, 'max_layers': 0}, <__main__.AngleDataset object at 0x000001F8DEB88508>), ('walk', {'lr': 1.1696658785595074e-10, 'momentum': 0.17941022107151428, 'batch_norm': False, 'dropout_rate': 0.587665906984705, 'epochs': 309, 'hidden_layers': 10, 'min_layers': 2, 'max_layers': 33}, <__main__.AngleDataset object at 0x000001F8DEC3F688>)]\n"
     ]
    }
   ],
   "source": [
    "angles_path = Path(\"Data\")\n",
    "names_conf_ds = []\n",
    "\n",
    "for filename in angles_path.glob(\"*_kinematic.csv\"):\n",
    "\n",
    "    gait_name = filename.stem.split(\"_\")[0]\n",
    "\n",
    "    f = open(TMP_PATH / f\"{gait_name}_best_config.json\")\n",
    "    config  = json.load(f)\n",
    "\n",
    "    ds = create_datasets(DATA_PATH / f'{gait_name}_kinematic.csv', nosplit=True)\n",
    "\n",
    "    names_conf_ds.append((gait_name, config, ds))\n",
    "\n",
    "print(names_conf_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "We'll use Ray for this and are using the search space below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "\n",
    "    \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9),\n",
    "    \"batch_norm\": tune.choice([True, False]),\n",
    "    \"dropout_rate\": tune.uniform(0.0, 0.6),\n",
    "    \"epochs\": tune.randint(5, 1000),\n",
    "    \"hidden_layers\": tune.choice([1, 2, 3, 5, 10, 20, 50]),\n",
    "    \"min_layers\": tune.choice([2, 28]),\n",
    "    \"max_layers\": tune.choice([0, 33, 50])\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Run Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = create_datasets(DATA_PATH / f'canter_kinematic.csv', nosplit=True)\n",
    "AVGS_KEY = \"canter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ray(config):\n",
    "    # epochs=4, layer_sizes=[33, 31, 30, 28], batch_norm=True, dropout=0):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 33\n",
    "\n",
    "    data_loader_train = DataLoader(\n",
    "        dataset=DATASET, batch_size=batch_size_train, shuffle=True\n",
    "    )\n",
    "    data_loader_test = DataLoader(\n",
    "        dataset=DATASET, batch_size=len(DATASET), shuffle=False\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameters\n",
    "    learning_rate = config[\"lr\"]\n",
    "    \n",
    "\n",
    "    # randomly decides on a bunch of layers depending on the search space decision\n",
    "    # layer_sizes = [batch_size_train]\n",
    "    # for _ in range(config[\"hidden_layers\"]):\n",
    "    #     min = config[\"min_layers\"]\n",
    "    #     max = config[\"max_layers\"] if config[\"max_layers\"] != 0 else layer_sizes[-1]\n",
    "    #     layer_sizes.append(random.randint(min, max))\n",
    "\n",
    "    # layer_sizes.append(28) #hard coded bc it's exactly the number we'll always need\n",
    "\n",
    "\n",
    "    layer_sizes=[33, 31, 30, 28]\n",
    "\n",
    "    pytorch_model = GaitModel(layer_sizes, config)\n",
    "\n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    for i in range(config[\"epochs\"]):\n",
    "        # Train and get the resulting loss per iteration\n",
    "        loss = train(\n",
    "            model=pytorch_model,\n",
    "            loader=data_loader_train,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Test and get the resulting predicted y values\n",
    "        y_predict, acc = test(model=pytorch_model, loader=data_loader_test)\n",
    "        tune.report(acc=acc)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gaits = [\"canter\", \"gallop\", \"trot\", \"walk\"]\n",
    "\n",
    "for gait in gaits:\n",
    "\n",
    "    AVGS_KEY = gait\n",
    "\n",
    "    DATASET = create_datasets(DATA_PATH / f'{gait}_kinematic.csv', nosplit=True)\n",
    "\n",
    "    # analysis = tune.run(run_ray, num_samples=20, scheduler=ASHAScheduler(metric=\"acc\", mode=\"max\"),config=search_space)\n",
    "\n",
    "    analysis = tune.run(run_ray, num_samples = 20, config=search_space)\n",
    "\n",
    "\n",
    "    best_config = analysis.get_best_config(metric=\"acc\", mode=\"max\")\n",
    "    json_config = json.dumps(best_config)\n",
    "\n",
    "    with open(TMP_PATH / f'{gait}_best_config.json', 'w') as outfile:\n",
    "        outfile.write(json_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Final Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config, train_dataset, test_dataset, avgs_key, layer_sizes=[33, 31, 30, 28]):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 33\n",
    "\n",
    "    data_loader_train = DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size_train, shuffle=True\n",
    "    )\n",
    "    data_loader_test = DataLoader(\n",
    "        dataset=test_dataset, batch_size=len(test_dataset), shuffle=False\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameters\n",
    "    learning_rate = config['lr']\n",
    "\n",
    "    AVGS_KEY = avgs_key\n",
    "\n",
    "    pytorch_model = GaitModel(layer_sizes, config)\n",
    "\n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    for i in range(config[\"epochs\"]):\n",
    "        # Train and get the resulting loss per iteration\n",
    "        loss = train(\n",
    "            model=pytorch_model,\n",
    "            loader=data_loader_train,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=pytorch_model, loader=data_loader_test)\n",
    "\n",
    "    return loss, y_predict, pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canter\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\2019271230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtrain_csv_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_conf_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\2019271230.py\u001b[0m in \u001b[0;36mtrain_csv_plot\u001b[1;34m(names_conf_ds, model_path, csv_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m         )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf'{name}_model.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\1355330471.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0my_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\1355330471.py\u001b[0m in \u001b[0;36mtest_batch\u001b[1;34m(model, x, y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# run forward calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\1455958988.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mX2\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "def train_csv_plot(names_conf_ds, model_path=MODEL_PATH, csv_path=DATA_PATH):\n",
    "\n",
    "    final_losses = []\n",
    "\n",
    "    for name, config, ds in names_conf_ds:\n",
    "        print(name)\n",
    "\n",
    "        losses, y_predict, model_to_save = run(config,\n",
    "            train_dataset=ds, test_dataset=ds, avgs_key=name,\n",
    "        )\n",
    "\n",
    "        y_predict = test(model_to_save, ds)\n",
    "\n",
    "        with open(csv_path / f'{name}_model.csv', \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(\n",
    "                f,\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "            )\n",
    "\n",
    "            writer.writerow(CSV_HEADER)\n",
    "            for row in y_predict:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        torch.save(model_to_save, model_path / f\"{name}_model.pt\")\n",
    "\n",
    "        final_loss = sum(losses[-100:])/100\n",
    "        final_losses.append(final_loss)\n",
    "\n",
    "        print(f\"Final loss for {name}: {final_loss}\")\n",
    "        \n",
    "        plot_loss(losses, name)\n",
    "\n",
    "    return sum(final_losses)/len(final_losses)\n",
    "\n",
    "        \n",
    "\n",
    "train_csv_plot(names_conf_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to .Py File\n",
    "Reminder to do this every time the notebook is ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading gait_model.ipynb in format ipynb\n",
      "[jupytext] Updating notebook metadata with '{\"jupytext\": {\"formats\": \"ipynb,py:percent\"}}'\n",
      "[jupytext] Updating gait_model.py\n"
     ]
    }
   ],
   "source": [
    "!jupytext --set-formats ipynb,py:percent gait_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e96169c3d85a0b7abfdf8329564d63e23581ffb90e909b0554daf0689ed6e7b"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
