{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'USING pytorch VERSION: ', torch.__version__: '1.6.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('USING pytorch VERSION: ', '1.6.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "ic(\"USING pytorch VERSION: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our data is in numpy format, which conveniently pytorch has a method for converting to their native format.\n",
    "\n",
    "The init function takes the path to the csv and creates a dataset out of it. I actually have three different options here. The dataset could be composed such that x is the 'timestamp' of the movement,the previous set of angles, or a tuple of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 33])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor     # for MSE or L1 Loss\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def create_datasets(csv_path: str, train_perc: float = 0.8):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    length = len(df)\n",
    "    time = 10\n",
    "    timestep = 0.005\n",
    "\n",
    "    # x_data = np.array([])\n",
    "    # y_data = np.array([])\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    sin_test_timepoints = np.random.rand(length, 1)*time   # Repeat data generation for test set\n",
    "    sin_test_timepoints = sin_test_timepoints.ravel()\n",
    "    sin_iter = iter(sin_test_timepoints)\n",
    "    \n",
    "    #data order = sin, angles, torso, touch_sens\n",
    "\n",
    "    # if x = curr angles and y = next angles\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        if i < length - 1:\n",
    "            x = np.append(x, df.iloc[i])\n",
    "            y = np.append(y, df.iloc[i+1][:-4]) # only include angles\n",
    "        else:\n",
    "            #since it loops anyway\n",
    "            x = np.append(x, df.iloc[i])\n",
    "            y = np.append(y, df.iloc[0][:-4])\n",
    "\n",
    "        x = np.append([next(sin_iter)], x)\n",
    "        \n",
    "        \n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    \n",
    "  \n",
    "    x_data = np.array(x_data, dtype=np.float64)\n",
    "    y_data = np.array(y_data, dtype=np.float64)\n",
    "\n",
    "    last_train_idx = int(len(x_data) * train_perc)\n",
    "\n",
    "    train_x = x_data[:last_train_idx]\n",
    "    train_y = y_data[:last_train_idx]\n",
    "    test_x = x_data[last_train_idx:]\n",
    "    test_y = y_data[last_train_idx:]\n",
    "\n",
    "    return AngleDataset(x=train_x, y=train_y), AngleDataset(x=test_x, y=test_y)\n",
    "\n",
    "train_dataset, test_dataset = create_datasets('./walk_angles.csv')\n",
    "\n",
    "train_dataset.x_data.shape\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training methods for the model\n",
    "These methods use an initialized model and training data to iteratively perform the forward and backward pass of optimization. Aside from some data reformatting that depends on the input, output, and loss function, these methods will always be the same for any shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, x, y, optimizer, loss_fn):\n",
    "    # Run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn, epochs=5):\n",
    "    losses = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for e in range(epochs):\n",
    "        for x, y in loader:\n",
    "            loss = train_batch(model=model, x=x, y=y, optimizer=optimizer, loss_fn=loss_fn)\n",
    "            losses.append(loss)\n",
    "\n",
    "            batch_index += 1\n",
    "\n",
    "        if e % 50 == 0:\n",
    "          ic(\"Epoch: \", e+1)\n",
    "          ic(\"Batches: \", batch_index)\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define testing methods for the model\n",
    "These methods are like training, but we don't need to update the parameters of the model anymore because when we call the test() method, the model has already been trained. Instead, this method just calculates the predicted y values and returns them, AKA the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, x, y):\n",
    "    # run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    return y, y_predict\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    y_vectors = list()\n",
    "    y_predict_vectors = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for x, y in loader:\n",
    "        y, y_predict = test_batch(model=model, x=x, y=y)\n",
    "\n",
    "        y_vectors.append(y.data.numpy())\n",
    "        y_predict_vectors.append(y_predict.data.numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "\n",
    "    y_predict_vector = np.concatenate(y_predict_vectors)\n",
    "\n",
    "    return y_predict_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting method for loss\n",
    "This is a plotting method for looking at the behavior of the loss over training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, show=True):\n",
    "    fig = pyplot.gcf()\n",
    "    fig.set_size_inches(8,6)\n",
    "    ax = pyplot.axes()\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    x_loss = list(range(len(losses)))\n",
    "    pyplot.plot(x_loss, losses)\n",
    "\n",
    "    if show:\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture\n",
    "- 33 inputs = 3 joint angles per leg, 4 legs, 2 DOF per joint. 4 touch sensors. 1 sine timestamp.\n",
    "- 28 outputs = *same as above, except just the joint angles*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitModel(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(GaitModel, self).__init__()\n",
    "\n",
    "\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU(), nn.BatchNorm1d(nl))\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer does not include an activation function.\n",
    "        # See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        tanh = torch.nn.Tanh()\n",
    "        \n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer] + [tanh]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now deprecated in favor of variable length NN (above)\n",
    "class PytorchBrain(torch.nn.Module):\n",
    "    _id = 0 # Global genome identifier\n",
    "    _num_inputs = 33\n",
    "    _num_outputs = 28\n",
    "\n",
    "    # @classmethod\n",
    "    # def __get_new_id(cls):\n",
    "    #     cls._id += 1\n",
    "    #     return cls._id\n",
    "\n",
    "    @classmethod\n",
    "    def get_num_outputs(cls):\n",
    "        return cls._num_outputs\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.h1 = torch.nn.Linear(PytorchBrain._num_inputs, 12)\n",
    "        self.relu = torch.nn.ReLU() #output is same shape as input\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(12)\n",
    "        self.h2 = torch.nn.Linear(12, 12)\n",
    "        self.h3 = torch.nn.Linear(12, 12)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = torch.nn.Linear(12, PytorchBrain._num_outputs)\n",
    "        \n",
    "        # Define sigmoid activation\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        # TODO: collapse all x = into \n",
    "        # TODO: either sigmoid OR Relu\n",
    "        x = self.h1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        # x = self.sigmoid(x)\n",
    "\n",
    "        x = self.h2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        # x = self.sigmoid(x)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        # x = self.sigmoid(x)\n",
    "\n",
    "        # TODO: linear then tanh might be better?\n",
    "        x = self.tanh(x) #caps output -1 to 1\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dataset, test_dataset, epochs=4, layer_sizes=[33, 31, 30, 28]):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 33\n",
    "    \n",
    "    data_loader_train = DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_test = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    pytorch_model = GaitModel(layer_sizes)\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss = train(model=pytorch_model, loader=data_loader_train, optimizer=optimizer, loss_fn=loss_fn, epochs=epochs)\n",
    "    \n",
    "\n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=pytorch_model, loader=data_loader_test)\n",
    "\n",
    "    return loss, y_predict, pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| train_dataset.length: 75\n",
      "ic| test_dataset.length: 19\n",
      "F:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([33, 24])) that is different to the input size (torch.Size([33, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (24) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18648\\1823731958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_to_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18648\\98966871.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(train_dataset, test_dataset, epochs, layer_sizes)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Train and get the resulting loss per iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18648\\2525942660.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18648\\2525942660.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(model, x, y, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Compute loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Before the backward pass, use the optimizer object to zero all of the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2646\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2647\u001b[1;33m         \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2648\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2649\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (28) must match the size of tensor b (24) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "ic(train_dataset.length)\n",
    "ic(test_dataset.length)\n",
    "\n",
    "losses, y_predict, model_to_save = run(train_dataset=train_dataset, test_dataset=test_dataset, epochs=400)\n",
    "torch.save(model_to_save, 'model.pt')\n",
    "\n",
    "ic(\"Final loss:\", sum(losses[-100:])/100)\n",
    "plot_loss(losses)\n",
    "\n",
    "# Transpose the matrices so we can plot.\n",
    "# y_test = test_dataset.y_data.transpose()\n",
    "# y_predict = y_predict.transpose()\n",
    "y_test = np.transpose(test_dataset.y_data)\n",
    "y_predict = np.transpose(y_predict)\n",
    "\n",
    "\n",
    "# for yt, yp in zip(y_test, y_predict):\n",
    "#   fig2 = pyplot.figure(dpi=300)\n",
    "#   fig2.set_size_inches(8,6)\n",
    "#   ic(test_dataset.x_data.shape)\n",
    "#   ic(yt.shape)\n",
    "#   ic(yp.shape)\n",
    "#   pyplot.scatter(test_dataset.x_data, yt, marker='o', s=0.2)\n",
    "#   ic('second scatter')\n",
    "#   pyplot.scatter(test_dataset.x_data, yp, marker='o', s=0.3)\n",
    "#   pyplot.text(-9, 0.44, \"- Prediction\", color=\"orange\", fontsize=8)\n",
    "#   pyplot.text(-9, 0.48, \"- Sine (with noise)\", color=\"blue\", fontsize=8)\n",
    "#   pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3299989700317383,\n",
       " 2.2911760807037354,\n",
       " 2.493915557861328,\n",
       " 2.4274396896362305,\n",
       " 2.3643479347229004,\n",
       " 2.3504559993743896,\n",
       " 2.2515764236450195,\n",
       " 2.369004249572754,\n",
       " 2.2836692333221436,\n",
       " 2.208125114440918,\n",
       " 2.2971057891845703,\n",
       " 2.2098734378814697,\n",
       " 2.24356746673584,\n",
       " 2.0889909267425537,\n",
       " 2.168239116668701,\n",
       " 2.149796962738037,\n",
       " 2.2388055324554443,\n",
       " 2.130577325820923,\n",
       " 2.028994560241699,\n",
       " 2.1951258182525635,\n",
       " 2.1261837482452393,\n",
       " 2.0404958724975586,\n",
       " 2.0034430027008057,\n",
       " 2.052830934524536,\n",
       " 2.0382909774780273,\n",
       " 1.993867039680481,\n",
       " 2.049422264099121,\n",
       " 1.9392725229263306,\n",
       " 1.919783115386963,\n",
       " 1.9625155925750732,\n",
       " 1.8850502967834473,\n",
       " 1.970686435699463,\n",
       " 1.8677446842193604,\n",
       " 1.9322062730789185,\n",
       " 1.9262807369232178,\n",
       " 1.7983603477478027,\n",
       " 1.7795259952545166,\n",
       " 1.8274887800216675,\n",
       " 1.8282990455627441,\n",
       " 1.780476450920105,\n",
       " 1.9193055629730225,\n",
       " 1.7257955074310303,\n",
       " 1.8326306343078613,\n",
       " 1.7721259593963623,\n",
       " 1.6594632863998413,\n",
       " 1.662449836730957,\n",
       " 1.7343165874481201,\n",
       " 1.7193888425827026,\n",
       " 1.746010422706604,\n",
       " 1.6545414924621582,\n",
       " 1.7160170078277588,\n",
       " 1.6124290227890015,\n",
       " 1.5078428983688354,\n",
       " 1.7087517976760864,\n",
       " 1.6171700954437256,\n",
       " 1.65498685836792,\n",
       " 1.5938677787780762,\n",
       " 1.6685937643051147,\n",
       " 1.4945682287216187,\n",
       " 1.4754395484924316,\n",
       " 1.4808653593063354,\n",
       " 1.5709198713302612,\n",
       " 1.5431967973709106,\n",
       " 1.510838508605957,\n",
       " 1.44578218460083,\n",
       " 1.5262185335159302,\n",
       " 1.4167742729187012,\n",
       " 1.4285454750061035,\n",
       " 1.4672765731811523,\n",
       " 1.5065770149230957,\n",
       " 1.4671708345413208,\n",
       " 1.3959938287734985,\n",
       " 1.3760628700256348,\n",
       " 1.3799939155578613,\n",
       " 1.4278253316879272,\n",
       " 1.3511464595794678,\n",
       " 1.413164496421814,\n",
       " 1.362188458442688,\n",
       " 1.292380690574646,\n",
       " 1.479026436805725,\n",
       " 1.3070898056030273,\n",
       " 1.3530116081237793,\n",
       " 1.2897686958312988,\n",
       " 1.2520885467529297,\n",
       " 1.303484320640564,\n",
       " 1.2838696241378784,\n",
       " 1.3047120571136475,\n",
       " 1.2581593990325928,\n",
       " 1.2215112447738647,\n",
       " 1.2543946504592896,\n",
       " 1.1841810941696167,\n",
       " 1.259568214416504,\n",
       " 1.22963547706604,\n",
       " 1.2422200441360474,\n",
       " 1.2045189142227173,\n",
       " 1.2027901411056519,\n",
       " 1.201081395149231,\n",
       " 1.123874545097351,\n",
       " 1.1512293815612793,\n",
       " 1.2206673622131348,\n",
       " 1.206275463104248,\n",
       " 1.1271148920059204,\n",
       " 1.122552514076233,\n",
       " 1.0958257913589478,\n",
       " 1.118088722229004,\n",
       " 1.0839749574661255,\n",
       " 1.1996835470199585,\n",
       " 1.1284269094467163,\n",
       " 1.0645986795425415,\n",
       " 1.0873711109161377,\n",
       " 1.0882564783096313,\n",
       " 1.083316445350647,\n",
       " 1.0659470558166504,\n",
       " 1.1177494525909424,\n",
       " 1.0781198740005493,\n",
       " 1.1105176210403442,\n",
       " 1.0369902849197388,\n",
       " 1.0138314962387085,\n",
       " 0.9925192594528198,\n",
       " 1.0155813694000244,\n",
       " 1.0134745836257935,\n",
       " 0.9447329044342041,\n",
       " 0.9872655868530273,\n",
       " 1.0034703016281128,\n",
       " 1.048520565032959,\n",
       " 0.9933209419250488,\n",
       " 0.988754153251648,\n",
       " 1.0055091381072998,\n",
       " 0.9283082485198975,\n",
       " 0.969588577747345,\n",
       " 0.9807601571083069,\n",
       " 0.9252831935882568,\n",
       " 0.9251700639724731,\n",
       " 0.9617841243743896,\n",
       " 0.9310208559036255,\n",
       " 0.9181962609291077,\n",
       " 0.850843071937561,\n",
       " 0.9911591410636902,\n",
       " 0.8414723873138428,\n",
       " 0.9299139380455017,\n",
       " 0.9394400715827942,\n",
       " 0.854135274887085,\n",
       " 0.9681580066680908,\n",
       " 0.8474873304367065,\n",
       " 0.8909167051315308,\n",
       " 0.8889603018760681,\n",
       " 0.8781351447105408,\n",
       " 0.8535343408584595,\n",
       " 0.8470975160598755,\n",
       " 0.8502439260482788,\n",
       " 0.9122800827026367,\n",
       " 0.8565811514854431,\n",
       " 0.8209195733070374,\n",
       " 0.7794181704521179,\n",
       " 0.8188254237174988,\n",
       " 0.8684808611869812,\n",
       " 0.8284516334533691,\n",
       " 0.8828809857368469,\n",
       " 0.8025488257408142,\n",
       " 0.7590671181678772,\n",
       " 0.7867171764373779,\n",
       " 0.8429734110832214,\n",
       " 0.7739184498786926,\n",
       " 0.7879022359848022,\n",
       " 0.8253211975097656,\n",
       " 0.765309751033783,\n",
       " 0.7948808670043945,\n",
       " 0.7986336946487427,\n",
       " 0.7974545359611511,\n",
       " 0.7136293053627014,\n",
       " 0.762831449508667,\n",
       " 0.7831428050994873,\n",
       " 0.7899357080459595,\n",
       " 0.762514054775238,\n",
       " 0.7420097589492798,\n",
       " 0.7231699228286743,\n",
       " 0.7536917924880981,\n",
       " 0.729123055934906,\n",
       " 0.7460610866546631,\n",
       " 0.8002089858055115,\n",
       " 0.6755876541137695,\n",
       " 0.7629193663597107,\n",
       " 0.7600042819976807,\n",
       " 0.7518657445907593,\n",
       " 0.7310090661048889,\n",
       " 0.6993927359580994,\n",
       " 0.7478639483451843,\n",
       " 0.7163124084472656,\n",
       " 0.7072389125823975,\n",
       " 0.7224411368370056,\n",
       " 0.6686347126960754,\n",
       " 0.7079092264175415,\n",
       " 0.691157341003418,\n",
       " 0.7082383632659912,\n",
       " 0.7358389496803284,\n",
       " 0.6534000635147095,\n",
       " 0.6848728060722351,\n",
       " 0.6858674883842468,\n",
       " 0.6480547189712524,\n",
       " 0.720672070980072,\n",
       " 0.690805196762085,\n",
       " 0.6522364616394043,\n",
       " 0.6623037457466125,\n",
       " 0.7052631974220276,\n",
       " 0.6478824019432068,\n",
       " 0.6737988591194153,\n",
       " 0.6891462802886963,\n",
       " 0.6666138172149658,\n",
       " 0.6753143072128296,\n",
       " 0.6364845037460327,\n",
       " 0.5998633503913879,\n",
       " 0.7011972069740295,\n",
       " 0.6811171770095825,\n",
       " 0.6893822550773621,\n",
       " 0.6387465000152588,\n",
       " 0.6097189784049988,\n",
       " 0.6244800090789795,\n",
       " 0.6888828873634338,\n",
       " 0.6434056162834167,\n",
       " 0.6269054412841797,\n",
       " 0.6211383938789368,\n",
       " 0.6556145548820496,\n",
       " 0.6099557280540466,\n",
       " 0.6438741087913513,\n",
       " 0.6522696018218994,\n",
       " 0.630542516708374,\n",
       " 0.5671135783195496,\n",
       " 0.673454999923706,\n",
       " 0.6318551301956177,\n",
       " 0.58414626121521,\n",
       " 0.6312418580055237,\n",
       " 0.6108864545822144,\n",
       " 0.5975437164306641,\n",
       " 0.6558930277824402,\n",
       " 0.6378669142723083,\n",
       " 0.5987548828125,\n",
       " 0.581119954586029,\n",
       " 0.6434438228607178,\n",
       " 0.5949437618255615,\n",
       " 0.6034032702445984,\n",
       " 0.6344824433326721,\n",
       " 0.5908999443054199,\n",
       " 0.6208401322364807,\n",
       " 0.5970962047576904,\n",
       " 0.5734597444534302,\n",
       " 0.5766814947128296,\n",
       " 0.5672127604484558,\n",
       " 0.5973503589630127,\n",
       " 0.5810513496398926,\n",
       " 0.584770917892456,\n",
       " 0.6435432434082031,\n",
       " 0.5807424187660217,\n",
       " 0.5823929309844971,\n",
       " 0.6024855971336365,\n",
       " 0.567355215549469,\n",
       " 0.5792098045349121,\n",
       " 0.6218502521514893,\n",
       " 0.5746573805809021,\n",
       " 0.5830569863319397,\n",
       " 0.567255437374115,\n",
       " 0.6200564503669739,\n",
       " 0.5672290325164795,\n",
       " 0.5566977262496948,\n",
       " 0.5797942280769348,\n",
       " 0.58149653673172,\n",
       " 0.568791389465332,\n",
       " 0.5468428134918213,\n",
       " 0.6107010245323181,\n",
       " 0.5762264728546143,\n",
       " 0.5510565042495728,\n",
       " 0.5802333950996399,\n",
       " 0.5465577840805054,\n",
       " 0.5953837037086487,\n",
       " 0.5944317579269409,\n",
       " 0.543342113494873,\n",
       " 0.5490197539329529,\n",
       " 0.5917404890060425,\n",
       " 0.5381159782409668,\n",
       " 0.5621311068534851,\n",
       " 0.5668070912361145,\n",
       " 0.5462150573730469,\n",
       " 0.588510274887085,\n",
       " 0.5658981204032898,\n",
       " 0.5611386299133301,\n",
       " 0.5570842623710632,\n",
       " 0.5333546996116638,\n",
       " 0.5352121591567993,\n",
       " 0.5989282131195068,\n",
       " 0.569133996963501,\n",
       " 0.5327615737915039,\n",
       " 0.551388680934906,\n",
       " 0.5490089058876038,\n",
       " 0.5792802572250366,\n",
       " 0.5343188047409058,\n",
       " 0.5664218664169312,\n",
       " 0.5403397679328918,\n",
       " 0.5211809873580933,\n",
       " 0.5095334649085999,\n",
       " 0.5742380023002625,\n",
       " 0.5792143940925598,\n",
       " 0.5553080439567566,\n",
       " 0.5230686664581299,\n",
       " 0.5444706082344055,\n",
       " 0.5782347321510315,\n",
       " 0.49734804034233093,\n",
       " 0.5779092311859131,\n",
       " 0.5286499261856079,\n",
       " 0.5530001521110535,\n",
       " 0.5469531416893005,\n",
       " 0.5832130908966064,\n",
       " 0.5392096042633057,\n",
       " 0.5148881077766418,\n",
       " 0.5728340148925781,\n",
       " 0.5160407423973083,\n",
       " 0.5256936550140381,\n",
       " 0.5567117929458618,\n",
       " 0.5207186341285706,\n",
       " 0.5660966634750366,\n",
       " 0.5478037595748901,\n",
       " 0.5483946204185486,\n",
       " 0.5205491185188293,\n",
       " 0.5370347499847412,\n",
       " 0.5020887851715088,\n",
       " 0.5518807172775269,\n",
       " 0.5296385884284973,\n",
       " 0.5464484095573425,\n",
       " 0.5562742948532104,\n",
       " 0.5391655564308167,\n",
       " 0.5118163228034973,\n",
       " 0.5247154831886292,\n",
       " 0.5371758937835693,\n",
       " 0.5066468715667725,\n",
       " 0.5454943776130676,\n",
       " 0.5209481716156006,\n",
       " 0.5491361021995544,\n",
       " 0.5236215591430664,\n",
       " 0.5393695831298828,\n",
       " 0.5365766882896423,\n",
       " 0.5485262870788574,\n",
       " 0.5223078727722168,\n",
       " 0.4967012405395508,\n",
       " 0.5511482954025269,\n",
       " 0.5349838733673096,\n",
       " 0.5130069255828857,\n",
       " 0.5278763175010681,\n",
       " 0.5192596912384033,\n",
       " 0.5615861415863037,\n",
       " 0.5155126452445984,\n",
       " 0.5100529193878174,\n",
       " 0.5665382146835327,\n",
       " 0.5075179934501648,\n",
       " 0.5316919088363647,\n",
       " 0.506364107131958,\n",
       " 0.5204678177833557,\n",
       " 0.5293285846710205,\n",
       " 0.5249590873718262,\n",
       " 0.5382379293441772,\n",
       " 0.5387243628501892,\n",
       " 0.5277068018913269,\n",
       " 0.4818864166736603,\n",
       " 0.49212032556533813,\n",
       " 0.5084540843963623,\n",
       " 0.5151199102401733,\n",
       " 0.5359198451042175,\n",
       " 0.5585776567459106,\n",
       " 0.5424187779426575,\n",
       " 0.49100610613822937,\n",
       " 0.5342563986778259,\n",
       " 0.5246661305427551,\n",
       " 0.5109492540359497,\n",
       " 0.5160232782363892,\n",
       " 0.5455713868141174,\n",
       " 0.5091795921325684,\n",
       " 0.4824502766132355,\n",
       " 0.5098520517349243,\n",
       " 0.5484480261802673,\n",
       " 0.5092912912368774,\n",
       " 0.565597414970398,\n",
       " 0.5275205969810486,\n",
       " 0.5019533634185791,\n",
       " 0.5283846855163574,\n",
       " 0.4974463880062103,\n",
       " 0.539735734462738,\n",
       " 0.5209442973136902,\n",
       " 0.4806632995605469,\n",
       " 0.4780881702899933,\n",
       " 0.528643012046814,\n",
       " 0.5504454970359802,\n",
       " 0.5389759540557861,\n",
       " 0.5312105417251587,\n",
       " 0.5119470953941345,\n",
       " 0.5199660062789917,\n",
       " 0.550660252571106,\n",
       " 0.5233572125434875,\n",
       " 0.4888263940811157,\n",
       " 0.5083048939704895,\n",
       " 0.5186336040496826,\n",
       " 0.5207034349441528,\n",
       " 0.5234166383743286,\n",
       " 0.515701413154602,\n",
       " 0.48618173599243164,\n",
       " 0.5394948720932007,\n",
       " 0.544004499912262,\n",
       " 0.5371353626251221,\n",
       " 0.49595367908477783,\n",
       " 0.5166504979133606,\n",
       " 0.5032230615615845,\n",
       " 0.49676230549812317,\n",
       " 0.5048655271530151,\n",
       " 0.5326237082481384,\n",
       " 0.50187748670578,\n",
       " 0.5046217441558838,\n",
       " 0.4931374788284302,\n",
       " 0.5425009727478027,\n",
       " 0.5115865468978882,\n",
       " 0.5326645374298096,\n",
       " 0.5162513256072998,\n",
       " 0.48152023553848267,\n",
       " 0.5225633978843689,\n",
       " 0.5168758630752563,\n",
       " 0.541714608669281,\n",
       " 0.5040009021759033,\n",
       " 0.49936574697494507,\n",
       " 0.49956628680229187,\n",
       " 0.5320166945457458,\n",
       " 0.5074716806411743,\n",
       " 0.5079270005226135,\n",
       " 0.5112716555595398,\n",
       " 0.5302523374557495,\n",
       " 0.45073044300079346,\n",
       " 0.5096107721328735,\n",
       " 0.5636715888977051,\n",
       " 0.4647471010684967,\n",
       " 0.5042136311531067,\n",
       " 0.5434515476226807,\n",
       " 0.5126192569732666,\n",
       " 0.5377898216247559,\n",
       " 0.501860499382019,\n",
       " 0.4991924464702606,\n",
       " 0.5224372744560242,\n",
       " 0.5509609580039978,\n",
       " 0.4700409471988678,\n",
       " 0.4913230538368225,\n",
       " 0.5263463854789734,\n",
       " 0.4986584484577179,\n",
       " 0.5367259979248047,\n",
       " 0.5361208319664001,\n",
       " 0.49406179785728455,\n",
       " 0.4755909740924835,\n",
       " 0.5103764533996582,\n",
       " 0.530211865901947,\n",
       " 0.5278312563896179,\n",
       " 0.5164322257041931,\n",
       " 0.47452667355537415,\n",
       " 0.5214138627052307,\n",
       " 0.47668713331222534,\n",
       " 0.5203443765640259,\n",
       " 0.48473942279815674,\n",
       " 0.49879974126815796,\n",
       " 0.5369470119476318,\n",
       " 0.529441773891449,\n",
       " 0.493219792842865,\n",
       " 0.5372927784919739,\n",
       " 0.5308817625045776,\n",
       " 0.5294623374938965,\n",
       " 0.4912678301334381,\n",
       " 0.4457741975784302,\n",
       " 0.5079912543296814,\n",
       " 0.4978509545326233,\n",
       " 0.49541938304901123,\n",
       " 0.4894178807735443,\n",
       " 0.5360944867134094,\n",
       " 0.4969218075275421,\n",
       " 0.5389546155929565,\n",
       " 0.5014520287513733,\n",
       " 0.47715452313423157,\n",
       " 0.5498297214508057,\n",
       " 0.5211581587791443,\n",
       " 0.4678525924682617,\n",
       " 0.5500057935714722,\n",
       " 0.47970566153526306,\n",
       " 0.5201663970947266,\n",
       " 0.47605645656585693,\n",
       " 0.5135029554367065,\n",
       " 0.5409706234931946,\n",
       " 0.4937096834182739,\n",
       " 0.47345462441444397,\n",
       " 0.5253944396972656,\n",
       " 0.46690478920936584,\n",
       " 0.5081460475921631,\n",
       " 0.5367653369903564,\n",
       " 0.5268944501876831,\n",
       " 0.5096947550773621,\n",
       " 0.4951704442501068,\n",
       " 0.4813457727432251,\n",
       " 0.4868350028991699,\n",
       " 0.4984382092952728,\n",
       " 0.5569631457328796,\n",
       " 0.5394210815429688,\n",
       " 0.5291457176208496,\n",
       " 0.509248673915863,\n",
       " 0.5196981430053711,\n",
       " 0.4782201945781708,\n",
       " 0.4443974792957306,\n",
       " 0.5194332599639893,\n",
       " 0.5327597260475159,\n",
       " 0.5052666664123535,\n",
       " 0.4510357677936554,\n",
       " 0.4975697100162506,\n",
       " 0.5266608595848083,\n",
       " 0.5110775232315063,\n",
       " 0.5333096981048584,\n",
       " 0.5148270130157471,\n",
       " 0.5239261388778687,\n",
       " 0.5040510892868042,\n",
       " 0.4522366523742676,\n",
       " 0.49800819158554077,\n",
       " 0.48870033025741577,\n",
       " 0.5569900274276733,\n",
       " 0.506520688533783,\n",
       " 0.5074446201324463,\n",
       " 0.47843047976493835,\n",
       " 0.5407916307449341,\n",
       " 0.4631253480911255,\n",
       " 0.5184416174888611,\n",
       " 0.4754929542541504,\n",
       " 0.5022803544998169,\n",
       " 0.5385693311691284,\n",
       " 0.4834051728248596,\n",
       " 0.5052236914634705,\n",
       " 0.4959169924259186,\n",
       " 0.5070881247520447,\n",
       " 0.5177021026611328,\n",
       " 0.5188631415367126,\n",
       " 0.5439177751541138,\n",
       " 0.4733748435974121,\n",
       " 0.5208759903907776,\n",
       " 0.49929746985435486,\n",
       " 0.47668585181236267,\n",
       " 0.5038250684738159,\n",
       " 0.5045353770256042,\n",
       " 0.510524570941925,\n",
       " 0.5356324911117554,\n",
       " 0.5053322315216064,\n",
       " 0.4844565987586975,\n",
       " 0.47382280230522156,\n",
       " 0.5222631096839905,\n",
       " 0.48866695165634155,\n",
       " 0.49679094552993774,\n",
       " 0.4487761855125427,\n",
       " 0.5314849019050598,\n",
       " 0.5276813507080078,\n",
       " 0.5106769800186157,\n",
       " 0.5182039737701416,\n",
       " 0.46350833773612976,\n",
       " 0.5226382613182068,\n",
       " 0.47702959179878235,\n",
       " 0.527877688407898,\n",
       " 0.5200191736221313,\n",
       " 0.5540035367012024,\n",
       " 0.49017539620399475,\n",
       " 0.4701870381832123,\n",
       " 0.5126055479049683,\n",
       " 0.48691269755363464,\n",
       " 0.5032045841217041,\n",
       " 0.5240444540977478,\n",
       " 0.5134263634681702,\n",
       " 0.4821692705154419,\n",
       " 0.45545995235443115,\n",
       " 0.5556827783584595,\n",
       " 0.501591682434082,\n",
       " 0.5250365734100342,\n",
       " 0.47659170627593994,\n",
       " 0.4690553843975067,\n",
       " 0.5354393720626831,\n",
       " 0.49204838275909424,\n",
       " 0.5023418664932251,\n",
       " 0.5537877082824707,\n",
       " 0.48193347454071045,\n",
       " 0.45420604944229126,\n",
       " 0.5016055107116699,\n",
       " 0.4884029030799866,\n",
       " 0.4756340980529785,\n",
       " 0.5368539690971375,\n",
       " 0.5070047974586487,\n",
       " 0.5156164169311523,\n",
       " 0.49977996945381165,\n",
       " 0.49543941020965576,\n",
       " 0.5020621418952942,\n",
       " 0.4794580638408661,\n",
       " 0.49662336707115173,\n",
       " 0.5286762118339539,\n",
       " 0.465464323759079,\n",
       " 0.5238510370254517,\n",
       " 0.5004119277000427,\n",
       " 0.4774695932865143,\n",
       " 0.5425159931182861,\n",
       " 0.4647129774093628,\n",
       " 0.5233020186424255,\n",
       " 0.4835301339626312,\n",
       " 0.48285457491874695,\n",
       " 0.47645267844200134,\n",
       " 0.45918014645576477,\n",
       " 0.5280777812004089,\n",
       " 0.5515164732933044,\n",
       " 0.4866514503955841,\n",
       " 0.49491336941719055,\n",
       " 0.4979459345340729,\n",
       " 0.4695455729961395,\n",
       " 0.5436919331550598,\n",
       " 0.4980480968952179,\n",
       " 0.5154754519462585,\n",
       " 0.5504790544509888,\n",
       " 0.4780697226524353,\n",
       " 0.49113526940345764,\n",
       " 0.48504918813705444,\n",
       " 0.5000676512718201,\n",
       " 0.4986001253128052,\n",
       " 0.5467264652252197,\n",
       " 0.47942644357681274,\n",
       " 0.5309358239173889,\n",
       " 0.43983137607574463,\n",
       " 0.5378230214118958,\n",
       " 0.4834223687648773,\n",
       " 0.5206522941589355,\n",
       " 0.5022034049034119,\n",
       " 0.5038518905639648,\n",
       " 0.48462173342704773,\n",
       " 0.45055776834487915,\n",
       " 0.4960472881793976,\n",
       " 0.49558204412460327,\n",
       " 0.5477374196052551,\n",
       " 0.5036134123802185,\n",
       " 0.4864422082901001,\n",
       " 0.4615735411643982,\n",
       " 0.45438671112060547,\n",
       " 0.4802294373512268,\n",
       " 0.5241675972938538,\n",
       " 0.5243217349052429,\n",
       " 0.45102202892303467,\n",
       " 0.5118505358695984,\n",
       " 0.5011541843414307,\n",
       " 0.4952470362186432,\n",
       " 0.5098011493682861,\n",
       " 0.4820975065231323,\n",
       " 0.48443952202796936,\n",
       " 0.54230797290802,\n",
       " 0.46823132038116455,\n",
       " 0.526841402053833,\n",
       " 0.5003029108047485,\n",
       " 0.4486854374408722,\n",
       " 0.5273618698120117,\n",
       " 0.48132455348968506,\n",
       " 0.4721040725708008,\n",
       " 0.5188436508178711,\n",
       " 0.4651584029197693,\n",
       " 0.4944992661476135,\n",
       " 0.4763011336326599,\n",
       " 0.5341330170631409,\n",
       " 0.4674556255340576,\n",
       " 0.5000236630439758,\n",
       " 0.503246545791626,\n",
       " 0.4833873510360718,\n",
       " 0.501641035079956,\n",
       " 0.5054307579994202,\n",
       " 0.4867754876613617,\n",
       " 0.47005975246429443,\n",
       " 0.5142149329185486,\n",
       " 0.49831146001815796,\n",
       " 0.46472036838531494,\n",
       " 0.48925527930259705,\n",
       " 0.515937864780426,\n",
       " 0.5020062327384949,\n",
       " 0.4667527377605438,\n",
       " 0.4862102270126343,\n",
       " 0.5347287654876709,\n",
       " 0.4891979396343231,\n",
       " 0.49265509843826294,\n",
       " 0.48519548773765564,\n",
       " 0.5230594277381897,\n",
       " 0.4578787386417389,\n",
       " 0.49480533599853516,\n",
       " 0.5218443274497986,\n",
       " 0.45535558462142944,\n",
       " 0.5474584102630615,\n",
       " 0.48964324593544006,\n",
       " 0.47185203433036804,\n",
       " 0.4750747084617615,\n",
       " 0.46675941348075867,\n",
       " 0.5116168856620789,\n",
       " 0.5110375881195068,\n",
       " 0.4933381676673889,\n",
       " 0.47641944885253906,\n",
       " 0.46217644214630127,\n",
       " 0.48466336727142334,\n",
       " 0.49529123306274414,\n",
       " 0.46684232354164124,\n",
       " 0.4708072245121002,\n",
       " 0.5352706909179688,\n",
       " 0.45733344554901123,\n",
       " 0.5541893243789673,\n",
       " 0.4507260322570801,\n",
       " 0.528242826461792,\n",
       " 0.4844803214073181,\n",
       " 0.5263695120811462,\n",
       " 0.43581169843673706,\n",
       " 0.46757569909095764,\n",
       " 0.49185532331466675,\n",
       " 0.4977892339229584,\n",
       " 0.4735477864742279,\n",
       " 0.46959108114242554,\n",
       " 0.5376390814781189,\n",
       " 0.42407310009002686,\n",
       " 0.5169064998626709,\n",
       " 0.5107004046440125,\n",
       " 0.4966709315776825,\n",
       " 0.4539337754249573,\n",
       " 0.4690747857093811,\n",
       " 0.5249996185302734,\n",
       " 0.4890458583831787,\n",
       " 0.4728631377220154,\n",
       " 0.4762928783893585,\n",
       " 0.4815100133419037,\n",
       " 0.4872106909751892,\n",
       " 0.51646888256073,\n",
       " 0.5198110938072205,\n",
       " 0.4665246605873108,\n",
       " 0.476806104183197,\n",
       " 0.5179429650306702,\n",
       " 0.5317811369895935,\n",
       " 0.4337386190891266,\n",
       " 0.48958998918533325,\n",
       " 0.4928089380264282,\n",
       " 0.4757927656173706,\n",
       " 0.521401047706604,\n",
       " 0.5197634696960449,\n",
       " 0.46713876724243164,\n",
       " 0.4400571584701538,\n",
       " 0.42491158843040466,\n",
       " 0.493118554353714,\n",
       " 0.49589040875434875,\n",
       " 0.5085892677307129,\n",
       " 0.5172410011291504,\n",
       " 0.4784655272960663,\n",
       " 0.4530640244483948,\n",
       " 0.45575422048568726,\n",
       " 0.49916622042655945,\n",
       " 0.4821853041648865,\n",
       " 0.515709638595581,\n",
       " 0.4933362305164337,\n",
       " 0.46745574474334717,\n",
       " 0.4911695122718811,\n",
       " 0.466388076543808,\n",
       " 0.5083329081535339,\n",
       " 0.47150421142578125,\n",
       " 0.48802050948143005,\n",
       " 0.5044941902160645,\n",
       " 0.4993055760860443,\n",
       " 0.4761422574520111,\n",
       " 0.4672430753707886,\n",
       " 0.5163052678108215,\n",
       " 0.4508090019226074,\n",
       " 0.5100386738777161,\n",
       " 0.5618326663970947,\n",
       " 0.46026018261909485,\n",
       " 0.4490753710269928,\n",
       " 0.46487918496131897,\n",
       " 0.5098099112510681,\n",
       " 0.5020003318786621,\n",
       " 0.5327260494232178,\n",
       " 0.47702038288116455,\n",
       " 0.4540354013442993,\n",
       " 0.5025032162666321,\n",
       " 0.47534024715423584,\n",
       " 0.47458410263061523,\n",
       " 0.4564647078514099,\n",
       " 0.5100111961364746,\n",
       " 0.5225008726119995,\n",
       " 0.46766793727874756,\n",
       " 0.4585089683532715,\n",
       " 0.4309540390968323,\n",
       " 0.47667476534843445,\n",
       " 0.4681348204612732,\n",
       " 0.5130138993263245,\n",
       " 0.48760706186294556,\n",
       " 0.5254443883895874,\n",
       " 0.49030032753944397,\n",
       " 0.4940353035926819,\n",
       " 0.4175564646720886,\n",
       " 0.46583837270736694,\n",
       " 0.5227146148681641,\n",
       " 0.5160382986068726,\n",
       " 0.4726744294166565,\n",
       " 0.4480660855770111,\n",
       " 0.494812548160553,\n",
       " 0.4880191385746002,\n",
       " 0.486813485622406,\n",
       " 0.4671061038970947,\n",
       " 0.46750184893608093,\n",
       " 0.5000396966934204,\n",
       " 0.47337859869003296,\n",
       " 0.44419339299201965,\n",
       " 0.4996931254863739,\n",
       " 0.5070286989212036,\n",
       " 0.4737689793109894,\n",
       " 0.48245224356651306,\n",
       " 0.4831449091434479,\n",
       " 0.45661985874176025,\n",
       " 0.504071831703186,\n",
       " 0.4811766743659973,\n",
       " 0.47529903054237366,\n",
       " 0.4602375030517578,\n",
       " 0.5122697353363037,\n",
       " 0.5036938190460205,\n",
       " 0.44959184527397156,\n",
       " 0.4736771583557129,\n",
       " 0.5408313274383545,\n",
       " 0.4474809467792511,\n",
       " 0.4730575978755951,\n",
       " 0.46752670407295227,\n",
       " 0.463386595249176,\n",
       " 0.519149899482727,\n",
       " 0.5111677646636963,\n",
       " 0.43625444173812866,\n",
       " 0.4683162271976471,\n",
       " 0.46513259410858154,\n",
       " 0.5033057928085327,\n",
       " 0.553036093711853,\n",
       " 0.4739209711551666,\n",
       " 0.49150773882865906,\n",
       " 0.4426170885562897,\n",
       " 0.4597740173339844,\n",
       " 0.4977790117263794,\n",
       " 0.4948688745498657,\n",
       " 0.5428686141967773,\n",
       " 0.5368592739105225,\n",
       " 0.40490347146987915,\n",
       " 0.4715009927749634,\n",
       " 0.508942723274231,\n",
       " 0.4907206892967224,\n",
       " 0.4841064214706421,\n",
       " 0.44651269912719727,\n",
       " 0.500966489315033,\n",
       " 0.49612659215927124,\n",
       " 0.5016650557518005,\n",
       " 0.509617805480957,\n",
       " 0.4486146569252014,\n",
       " 0.498879075050354,\n",
       " 0.5414596199989319,\n",
       " 0.47785159945487976,\n",
       " 0.4515452980995178,\n",
       " 0.4842000901699066,\n",
       " 0.4777781665325165,\n",
       " 0.49824297428131104,\n",
       " 0.49398285150527954,\n",
       " 0.45396530628204346,\n",
       " 0.4627422094345093,\n",
       " 0.5201768279075623,\n",
       " 0.5203697681427002,\n",
       " 0.48985958099365234,\n",
       " 0.5038097500801086,\n",
       " 0.4243294596672058,\n",
       " 0.4190230369567871,\n",
       " 0.5049387216567993,\n",
       " 0.4881485402584076,\n",
       " 0.4702759385108948,\n",
       " 0.49880579113960266,\n",
       " 0.45250824093818665,\n",
       " 0.47640472650527954,\n",
       " 0.47223180532455444,\n",
       " 0.4655226469039917,\n",
       " 0.4487389028072357,\n",
       " 0.5290670394897461,\n",
       " 0.43870311975479126,\n",
       " 0.49031862616539,\n",
       " 0.4886690080165863,\n",
       " 0.5173714756965637,\n",
       " 0.5106204152107239,\n",
       " 0.48695623874664307,\n",
       " 0.45027467608451843,\n",
       " 0.46094539761543274,\n",
       " 0.4647098183631897,\n",
       " 0.49898990988731384,\n",
       " 0.43936270475387573,\n",
       " 0.47617000341415405,\n",
       " 0.4670974314212799,\n",
       " 0.5055302977561951,\n",
       " 0.4714607000350952,\n",
       " 0.488434374332428,\n",
       " 0.5098080635070801,\n",
       " 0.45196014642715454,\n",
       " 0.4858819246292114,\n",
       " 0.45663323998451233,\n",
       " 0.5004679560661316,\n",
       " 0.5052183866500854,\n",
       " 0.4855952262878418,\n",
       " 0.47810348868370056,\n",
       " 0.5027582049369812,\n",
       " 0.4240030348300934,\n",
       " 0.4811558425426483,\n",
       " 0.4909001588821411,\n",
       " 0.41898274421691895,\n",
       " 0.47829103469848633,\n",
       " 0.4512004256248474,\n",
       " 0.5274169445037842,\n",
       " 0.4683976173400879,\n",
       " 0.41790518164634705,\n",
       " 0.47933149337768555,\n",
       " 0.4756387174129486,\n",
       " 0.47821271419525146,\n",
       " 0.4888843894004822,\n",
       " 0.48946860432624817,\n",
       " 0.4917328655719757,\n",
       " 0.49507924914360046,\n",
       " 0.5105693340301514,\n",
       " 0.4899304211139679,\n",
       " 0.48123544454574585,\n",
       " 0.4450426399707794,\n",
       " 0.5109598636627197,\n",
       " 0.5337864756584167,\n",
       " 0.43372172117233276,\n",
       " 0.4316217601299286,\n",
       " 0.4379878640174866,\n",
       " 0.5159885287284851,\n",
       " 0.48261627554893494,\n",
       " 0.4897381365299225,\n",
       " 0.459004282951355,\n",
       " 0.5407169461250305,\n",
       " 0.4318503737449646,\n",
       " 0.43883562088012695,\n",
       " 0.4937015175819397,\n",
       " 0.4732634127140045,\n",
       " 0.45412397384643555,\n",
       " 0.5167209506034851,\n",
       " 0.4479767382144928,\n",
       " 0.4605995714664459,\n",
       " 0.5333669781684875,\n",
       " 0.5059363842010498,\n",
       " 0.4550246596336365,\n",
       " 0.4249628782272339,\n",
       " 0.49918535351753235,\n",
       " 0.5052918195724487,\n",
       " 0.4954551160335541,\n",
       " 0.43681877851486206,\n",
       " 0.49304354190826416,\n",
       " 0.4464123547077179,\n",
       " 0.5164997577667236,\n",
       " 0.49187007546424866,\n",
       " 0.466220498085022,\n",
       " 0.4479697644710541,\n",
       " 0.47215980291366577,\n",
       " 0.5270152688026428,\n",
       " 0.4437023997306824,\n",
       " 0.4847380220890045,\n",
       " 0.4635084867477417,\n",
       " 0.415115088224411,\n",
       " 0.5185760855674744,\n",
       " 0.43900611996650696,\n",
       " 0.5099871158599854,\n",
       " 0.4877064526081085,\n",
       " 0.49787548184394836,\n",
       " 0.4326344132423401,\n",
       " 0.4873688519001007,\n",
       " 0.4812169671058655,\n",
       " 0.44008922576904297,\n",
       " 0.4759133458137512,\n",
       " 0.5106565356254578,\n",
       " 0.42443931102752686,\n",
       " 0.4513930678367615,\n",
       " 0.5103447437286377,\n",
       " 0.43586206436157227,\n",
       " 0.4814246594905853,\n",
       " 0.4488852322101593,\n",
       " 0.4870086908340454,\n",
       " 0.43794187903404236,\n",
       " 0.47487396001815796,\n",
       " 0.5172688364982605,\n",
       " 0.44978436827659607,\n",
       " 0.5309228301048279,\n",
       " 0.43057066202163696,\n",
       " 0.5067456960678101,\n",
       " 0.4572776257991791,\n",
       " 0.4728540778160095,\n",
       " 0.4716634452342987,\n",
       " 0.4874451458454132,\n",
       " 0.44190192222595215,\n",
       " 0.49754926562309265,\n",
       " 0.45419108867645264,\n",
       " 0.43840491771698,\n",
       " 0.4168011546134949,\n",
       " 0.48552608489990234,\n",
       " 0.4770415127277374,\n",
       " 0.46316659450531006,\n",
       " 0.44540515542030334,\n",
       " 0.44731462001800537,\n",
       " 0.4851585626602173,\n",
       " 0.39552655816078186,\n",
       " 0.43816691637039185,\n",
       " 0.4163866937160492,\n",
       " 0.499163419008255,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e96169c3d85a0b7abfdf8329564d63e23581ffb90e909b0554daf0689ed6e7b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
