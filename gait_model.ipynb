{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Define-a-pytorch-Dataset-object-to-contain-the-training-and-testing-data\" data-toc-modified-id=\"Define-a-pytorch-Dataset-object-to-contain-the-training-and-testing-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Define a pytorch Dataset object to contain the training and testing data</a></span></li><li><span><a href=\"#Define-training-methods-for-the-model\" data-toc-modified-id=\"Define-training-methods-for-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Define training methods for the model</a></span></li><li><span><a href=\"#Define-testing-methods-for-the-model\" data-toc-modified-id=\"Define-testing-methods-for-the-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Define testing methods for the model</a></span></li><li><span><a href=\"#Define-plotting-method-for-loss\" data-toc-modified-id=\"Define-plotting-method-for-loss-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Define plotting method for loss</a></span></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define Model Architecture</a></span></li><li><span><a href=\"#Define-Run-function\" data-toc-modified-id=\"Define-Run-function-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Define Run function</a></span></li><li><span><a href=\"#Create-Datasets-for-Each-Gait-File\" data-toc-modified-id=\"Create-Datasets-for-Each-Gait-File-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Create Datasets for Each Gait File</a></span></li><li><span><a href=\"#Run-and-plot-results\" data-toc-modified-id=\"Run-and-plot-results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Run and plot results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import inf\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Constants and Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "lines_to_next_cell": 1,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ANIMATIONS_DIR = Path(\"Animations/\")\n",
    "DATA_DIR = Path(\"Data/\")\n",
    "SANITY_DIR = Path(\"Sanity_Checks/\")\n",
    "MODEL_OUTPUT_DIR = Path(\"Model_Outputs/\")\n",
    "MODEL_DIR = Path(\"Models/\")\n",
    "\n",
    "segment_names = [\"FL\", \"FR\", \"BL\", \"BR\", \"SP\"]\n",
    "joints_per_segment = (3, 3, 3, 3, 2)\n",
    "dof_per_join = 2\n",
    "\n",
    "CSV_HEADER = []\n",
    "for segment_name, num_joints in zip(segment_names, joints_per_segment):\n",
    "    for joint_index in range(num_joints):\n",
    "        for dof_index in range(dof_per_join):\n",
    "            CSV_HEADER.append(f\"{segment_name} A{joint_index + 1} DF {dof_index + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a\n",
    "wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our\n",
    "data is in numpy format, which conveniently pytorch has a method for converting to their native format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.length = x.shape[0]\n",
    "        self.x_data = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        self.y_data = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[float, float]:\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def create_datasets(\n",
    "    csv_path: str, train_perc: float = 0.8, split: bool = True, shuffle: bool = False\n",
    "):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    length = len(df)\n",
    "\n",
    "    # The network takes a sinusoidal signal as an input\n",
    "    sim_time = 10\n",
    "    freq = 1\n",
    "    time = np.linspace(0, sim_time, length)\n",
    "    sin_signal = np.sin(2 * np.pi * freq * time)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sin_signal)\n",
    "\n",
    "    # data order = sin (1), angles (4*3*2), torso (2*2), touch_sens (4)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for sin_val, i in zip(sin_signal, range(length)):\n",
    "        x_data.append(np.hstack([[sin_val], df.iloc[i]]))\n",
    "\n",
    "        # No need to predict/output touch sensor values\n",
    "        y_data.append(df.iloc[i + 1][:-4] if i < length - 1 else df.iloc[0][:-4])\n",
    "\n",
    "    x_data = np.array(x_data, dtype=np.float64)\n",
    "    y_data = np.array(y_data, dtype=np.float64)\n",
    "\n",
    "    if split:\n",
    "        last_train_idx = int(len(x_data) * train_perc)\n",
    "\n",
    "        train_x = x_data[:last_train_idx]\n",
    "        train_y = y_data[:last_train_idx]\n",
    "        test_x = x_data[last_train_idx:]\n",
    "        test_y = y_data[last_train_idx:]\n",
    "\n",
    "        # Return split data\n",
    "        return AngleDataset(x=train_x, y=train_y), AngleDataset(x=test_x, y=test_y)\n",
    "\n",
    "    else:\n",
    "        # Return all data\n",
    "        return AngleDataset(x=x_data, y=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_col_averages(kinematics_filename: str) -> list[float]:\n",
    "    df = pd.read_csv(kinematics_filename)\n",
    "    return ((df.min() + df.max()) / 2).tolist()\n",
    "\n",
    "\n",
    "avgs = {\n",
    "    f.stem.split(\"_\")[0]: get_col_averages(f) for f in DATA_DIR.glob(\"*_kinematic.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define training methods for the model\n",
    "\n",
    "These methods use an initialized model and training data to iteratively perform the forward and backward pass of\n",
    "optimization. Aside from some data reformatting that depends on the input, output, and loss function, these methods will\n",
    "always be the same for any shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(model, x, y, optimizer, loss_fn) -> float:\n",
    "    model.train()\n",
    "\n",
    "    # Compute output\n",
    "    y_predict = model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Zero out current gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn, num_epochs=5) -> list[float]:\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in loader:\n",
    "            loss = train_batch(model, x, y, optimizer, loss_fn)\n",
    "            losses.append(loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define testing methods for the model\n",
    "\n",
    "These methods are like training, but we don't need to update the parameters of the model anymore because when we call\n",
    "the test() method, the model has already been trained. Instead, this method just calculates the predicted y values and\n",
    "returns them, AKA the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_batch(model, x) -> float:\n",
    "    model.eval()\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "def test(model, loader) -> list[float]:\n",
    "    predictions = []\n",
    "\n",
    "    for x, y in loader:\n",
    "        y_predict = test_batch(model, x)\n",
    "        predictions.append(y_predict.data.numpy())\n",
    "\n",
    "    # y_predict_vector = np.concatenate(predictions)\n",
    "\n",
    "    return y_predict_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define plotting method for loss\n",
    "This is a plotting method for looking at the behavior of the loss over training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(losses, title: str, show=True):\n",
    "    fig = pyplot.gcf()\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax = pyplot.axes()\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    x_loss = list(range(len(losses)))\n",
    "    pyplot.title(title)\n",
    "\n",
    "    pyplot.plot(x_loss, losses)\n",
    "\n",
    "    if show:\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Model Architecture\n",
    "- 33 inputs = 3 joint angles per leg, 4 legs, 2 DOF per joint. 4 touch sensors. 1 sine timestamp.\n",
    "- 28 outputs = *same as above, except just the joint angles*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GaitModel(nn.Module):\n",
    "    def __init__(self, layer_sizes, avgs_key, batch_norm=True, dropout=0):\n",
    "        super(GaitModel, self).__init__()\n",
    "        self.avgs = torch.FloatTensor(avgs[avgs_key])\n",
    "        hidden_layers = []\n",
    "\n",
    "        for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes):\n",
    "            layers = [nn.Linear(nlminus1, nl), nn.ReLU()]\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(nl))\n",
    "\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            hidden_layers.append(nn.Sequential(*layers))\n",
    "\n",
    "        # The output layer does not include an activation function.\n",
    "        # See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # tanh = torch.nn.Tanh()\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        tmp = np.zeros(X.shape)\n",
    "        tmp[:, 0] = X[:, 0]  # first\n",
    "        X2 = X[:, 1:]\n",
    "        X2 -= self.avgs\n",
    "\n",
    "        tmp[:, 1:] = X2[:, :]\n",
    "        X = torch.FloatTensor(tmp)\n",
    "\n",
    "        X = self.layers(X)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp = np.zeros(X.shape)\n",
    "            X += self.avgs[4:]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    avgs_key,\n",
    "    epochs=4,\n",
    "    layer_sizes=[33, 31, 30, 28],\n",
    "    batch_norm=True,\n",
    "    dropout=0,\n",
    "):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 33\n",
    "\n",
    "    data_loader_train = DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size_train, shuffle=True\n",
    "    )\n",
    "    data_loader_test = DataLoader(\n",
    "        dataset=test_dataset, batch_size=len(test_dataset), shuffle=False\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    pytorch_model = GaitModel(layer_sizes, avgs_key, batch_norm, dropout)\n",
    "\n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss = train(\n",
    "        model=pytorch_model,\n",
    "        loader=data_loader_train,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        num_epochs=epochs,\n",
    "    )\n",
    "\n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=pytorch_model, loader=data_loader_test)\n",
    "\n",
    "    return loss, y_predict, pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create Datasets for Each Gait File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "angles_path = Path(\"Data\")\n",
    "names_and_ds = []\n",
    "\n",
    "for filename in angles_path.glob(\"*_kinematic.csv\"):\n",
    "    gait_name = filename.stem.split(\"_\")[0]\n",
    "    train_ds, test_ds = create_datasets(filename)\n",
    "    names_and_ds.append((gait_name, train_ds, test_ds))\n",
    "\n",
    "print(names_and_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_csv_plot(\n",
    "    names_and_ds,\n",
    "    model_path=MODEL_DIR,\n",
    "    csv_path=DATA_DIR,\n",
    "    plot=True,\n",
    "    dropout=0,\n",
    "    batch_norm=True,\n",
    "    extra_id: str = \"\",\n",
    "):\n",
    "    final_losses = []\n",
    "\n",
    "    for name, train_ds, test_ds in names_and_ds:\n",
    "        print(name)\n",
    "\n",
    "        train_ds.x_data.shape\n",
    "\n",
    "        losses, y_predict, model_to_save = run(\n",
    "            train_dataset=train_ds,\n",
    "            test_dataset=test_ds,\n",
    "            avgs_key=name,\n",
    "            epochs=400,\n",
    "            dropout=dropout,\n",
    "            batch_norm=batch_norm,\n",
    "        )\n",
    "\n",
    "        # new /\n",
    "        all_ds = create_datasets(DATA_DIR / f\"{name}_kinematic.csv\", split=True)\n",
    "\n",
    "        data_loader_all = DataLoader(dataset=all_ds, batch_size=33, shuffle=False)\n",
    "        y_predict = test(model_to_save, data_loader_all)\n",
    "\n",
    "        spacer = \"_\" if extra_id != \"\" else \"\"\n",
    "\n",
    "        # / new\n",
    "        # Save the outputs to a csv for quicker comparisons later\n",
    "        with open(\n",
    "            csv_path / f\"{name}_model{spacer}{extra_id}.csv\", \"w\", newline=\"\"\n",
    "        ) as f:\n",
    "            writer = csv.writer(\n",
    "                f,\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "            )\n",
    "\n",
    "            writer.writerow(CSV_HEADER)\n",
    "            for row in y_predict:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        torch.save(model_to_save, model_path / f\"{name}_model{spacer}{extra_id}.pt\")\n",
    "\n",
    "        final_loss = sum(losses[-100:]) / 100\n",
    "        final_losses.append(final_loss)\n",
    "\n",
    "        print(f\"Final loss for {name}: {final_loss}\")\n",
    "        if plot:\n",
    "            plot_loss(losses, name)\n",
    "\n",
    "    return sum(final_losses) / len(final_losses)\n",
    "\n",
    "\n",
    "train_csv_plot(names_and_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sanity Check Data By Plotting\n",
    "Also saving to files under Sanity_Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_files = sorted([x for x in DATA_DIR.glob(\"*_kinematic.csv\") if x.is_file()])\n",
    "model_outputs = sorted([x for x in DATA_DIR.glob(\"*_model.csv\") if x.is_file()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for actual, pred in zip(data_files, model_outputs):\n",
    "    dfa = pd.read_csv(actual)\n",
    "    dfp = pd.read_csv(pred)\n",
    "\n",
    "    plot_actual = dfa.plot(\n",
    "        subplots=True, layout=(6, 6), figsize=(16, 16), title=f\"{actual.stem}: actual\"\n",
    "    )\n",
    "    plot_predicted = dfp.plot(\n",
    "        subplots=True,\n",
    "        layout=(6, 6),\n",
    "        figsize=(16, 16),\n",
    "        title=f\"{pred.stem}: predicted by model\",\n",
    "    )\n",
    "\n",
    "    plot_actual[0][0].get_figure().savefig(\n",
    "        SANITY_DIR / f\"{actual.stem}_actual.png\", facecolor=\"white\"\n",
    "    )\n",
    "    plot_predicted[0][0].get_figure().savefig(\n",
    "        SANITY_DIR / f\"{pred.stem}_predicted.png\", facecolor=\"white\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparisons(data_files, model_outputs, save_path, extra_id: str = None):\n",
    "    for actual, pred in zip(data_files, model_outputs):\n",
    "\n",
    "        dfa = pd.read_csv(actual)\n",
    "        dfp = pd.read_csv(pred)\n",
    "\n",
    "        name = actual.stem.split(\"_\")[0]\n",
    "\n",
    "        fig, axs = pyplot.subplots(\n",
    "            nrows=6, ncols=10, sharey=True, sharex=True, figsize=(16, 12)\n",
    "        )\n",
    "        fig.suptitle(f\"{name}: actual (left) vs predicted (right)\", fontsize=16)\n",
    "        # fig.set_size_inches(9, 9, forward=True)\n",
    "        for ax_arr in axs:\n",
    "            for ax in ax_arr:\n",
    "                # ax.set_aspect('equal')\n",
    "                # ax.set_adjustable('box')\n",
    "                ax.set_box_aspect(1)\n",
    "        pyplot.subplots_adjust(\n",
    "            left=0.025, right=1.0, bottom=0.1, top=0.9, wspace=0.5, hspace=0.5\n",
    "        )\n",
    "        pyplot.ylim(-3, 3)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        dfa = dfa.iloc[:, :-4]\n",
    "\n",
    "        dfa_p1 = dfa.iloc[:, :6]\n",
    "        dfa_p2 = dfa.iloc[:, 6:12]\n",
    "        dfa_p3 = dfa.iloc[:, 12:18]\n",
    "        dfa_p4 = dfa.iloc[:, 18:24]\n",
    "        dfa_p5 = dfa.iloc[:, 24:]\n",
    "\n",
    "        dfp_p1 = dfp.iloc[:, :6]\n",
    "        dfp_p2 = dfp.iloc[:, 6:12]\n",
    "        dfp_p3 = dfp.iloc[:, 12:18]\n",
    "        dfp_p4 = dfp.iloc[:, 18:24]\n",
    "        dfp_p5 = dfp.iloc[:, 24:]\n",
    "\n",
    "        dfa_p1.plot(ax=axs[:, 0], subplots=True)\n",
    "        dfp_p1.plot(ax=axs[:, 1], subplots=True)\n",
    "        dfa_p2.plot(ax=axs[:, 2], subplots=True)\n",
    "        dfp_p2.plot(ax=axs[:, 3], subplots=True)\n",
    "        dfa_p3.plot(ax=axs[:, 4], subplots=True)\n",
    "        dfp_p3.plot(ax=axs[:, 5], subplots=True)\n",
    "        dfa_p4.plot(ax=axs[:, 6], subplots=True)\n",
    "        dfp_p4.plot(ax=axs[:, 7], subplots=True)\n",
    "        dfa_p5.plot(ax=axs[:-2, 8], subplots=True)\n",
    "        dfp_p5.plot(ax=axs[:-2, 9], subplots=True)\n",
    "\n",
    "        spacer = \"_\" if extra_id else \"\"\n",
    "\n",
    "        fig.savefig(\n",
    "            save_path / f\"{name}_comparison{spacer}{extra_id}\", facecolor=\"white\"\n",
    "        )\n",
    "\n",
    "\n",
    "# plot_comparisons(data_files, model_outputs, SANITY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"bothoff\", False, 0),\n",
    "    (\"onlydrop\", False, 0.5),\n",
    "    (\"bothon\", True, 0.5),\n",
    "    (\"onlybatch\", True, 0),\n",
    "]\n",
    "\n",
    "tmp_path = Path(\"TMP/\")\n",
    "\n",
    "names_and_ds = []\n",
    "\n",
    "for filename in DATA_DIR.glob(\"*_kinematic.csv\"):\n",
    "    gait_name = filename.stem.split(\"_\")[0]\n",
    "    train_ds, test_ds = create_datasets(filename)\n",
    "    names_and_ds.append((gait_name, train_ds, test_ds))\n",
    "\n",
    "min_loss = inf\n",
    "best_config = None\n",
    "\n",
    "for config, batch, drop in configs:\n",
    "\n",
    "    # trains all gait types w this config\n",
    "    avg_loss = train_csv_plot(\n",
    "        names_and_ds,\n",
    "        tmp_path,\n",
    "        tmp_path,\n",
    "        plot=False,\n",
    "        dropout=drop,\n",
    "        batch_norm=batch,\n",
    "        extra_id=config,\n",
    "    )\n",
    "\n",
    "    if min_loss > avg_loss:\n",
    "        min_loss = avg_loss\n",
    "        best_config = config\n",
    "\n",
    "    # now we've got a set of models trained w these specific configs and a csv for each. need to compare vs regular graphs\n",
    "    config_outputs = sorted(\n",
    "        [x for x in tmp_path.glob(f\"*_{config}.csv\") if x.is_file()]\n",
    "    )\n",
    "\n",
    "    plot_comparisons(\n",
    "        data_files, config_outputs, tmp_path, extra_id=config\n",
    "    )  # plot to tmp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bothoff\n"
     ]
    }
   ],
   "source": [
    "print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading gait_model.ipynb in format ipynb\n",
      "[jupytext] Updating notebook metadata with '{\"jupytext\": {\"formats\": \"ipynb,py:percent\"}}'\n",
      "[jupytext] Updating the timestamp of gait_model.py\n"
     ]
    }
   ],
   "source": [
    "!jupytext --set-formats ipynb, py:percent gait_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8667ff9c1a3fdb73137a6895aac8b28d03ed0e6b6a6c03fe473fbbb3abb38ca6"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
