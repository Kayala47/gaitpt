{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'USING pytorch VERSION: ', torch.__version__: '1.6.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('USING pytorch VERSION: ', '1.6.0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "import random\n",
    "\n",
    "ic(\"USING pytorch VERSION: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our data is in numpy format, which conveniently pytorch has a method for converting to their native format.\n",
    "\n",
    "The init function takes the path to the csv and creates a dataset out of it. I actually have three different options here. The dataset could be composed such that x is the 'timestamp' of the movement,the previous set of angles, or a tuple of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([76, 33])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor     # for MSE or L1 Loss\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def create_datasets(csv_path: str, train_perc: float = 0.8):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    length = len(df)\n",
    "    time = 10\n",
    "    timestep = 0.005\n",
    "\n",
    "    # x_data = np.array([])\n",
    "    # y_data = np.array([])\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    sin_test_timepoints = np.random.rand(length, 1)*time   # Repeat data generation for test set\n",
    "    sin_test_timepoints = sin_test_timepoints.ravel()\n",
    "    sin_iter = iter(sin_test_timepoints)\n",
    "    \n",
    "    #data order = sin, angles, torso, touch_sens\n",
    "\n",
    "    # if x = curr angles and y = next angles\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        if i < length - 1:\n",
    "            x = np.append(x, df.iloc[i])\n",
    "            y = np.append(y, df.iloc[i+1][:-4]) # only include angles\n",
    "        else:\n",
    "            #since it loops anyway\n",
    "            x = np.append(x, df.iloc[i])\n",
    "            y = np.append(y, df.iloc[0][:-4])\n",
    "\n",
    "        x = np.append([next(sin_iter)], x)\n",
    "        \n",
    "        \n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    \n",
    "  \n",
    "    x_data = np.array(x_data, dtype=np.float64)\n",
    "    y_data = np.array(y_data, dtype=np.float64)\n",
    "\n",
    "    last_train_idx = int(len(x_data) * train_perc)\n",
    "\n",
    "    train_x = x_data[:last_train_idx]\n",
    "    train_y = y_data[:last_train_idx]\n",
    "    test_x = x_data[last_train_idx:]\n",
    "    test_y = y_data[last_train_idx:]\n",
    "\n",
    "    return AngleDataset(x=train_x, y=train_y), AngleDataset(x=test_x, y=test_y)\n",
    "\n",
    "train_dataset, test_dataset = create_datasets('./walk_angles.csv')\n",
    "\n",
    "train_dataset.x_data.shape\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training methods for the model\n",
    "These methods use an initialized model and training data to iteratively perform the forward and backward pass of optimization. Aside from some data reformatting that depends on the input, output, and loss function, these methods will always be the same for any shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, x, y, optimizer, loss_fn):\n",
    "    # Run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn, epochs=5):\n",
    "    losses = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for e in range(epochs):\n",
    "        for x, y in loader:\n",
    "            loss = train_batch(model=model, x=x, y=y, optimizer=optimizer, loss_fn=loss_fn)\n",
    "            losses.append(loss)\n",
    "\n",
    "            batch_index += 1\n",
    "\n",
    "        if e % 50 == 0:\n",
    "          ic(\"Epoch: \", e+1)\n",
    "          ic(\"Batches: \", batch_index)\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define testing methods for the model\n",
    "These methods are like training, but we don't need to update the parameters of the model anymore because when we call the test() method, the model has already been trained. Instead, this method just calculates the predicted y values and returns them, AKA the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, x, y):\n",
    "    # run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    return y, y_predict\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    y_vectors = list()\n",
    "    y_predict_vectors = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for x, y in loader:\n",
    "        y, y_predict = test_batch(model=model, x=x, y=y)\n",
    "\n",
    "        y_vectors.append(y.data.numpy())\n",
    "        y_predict_vectors.append(y_predict.data.numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "\n",
    "    y_predict_vector = np.concatenate(y_predict_vectors)\n",
    "\n",
    "    return y_predict_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting method for loss\n",
    "This is a plotting method for looking at the behavior of the loss over training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, show=True):\n",
    "    fig = pyplot.gcf()\n",
    "    fig.set_size_inches(8,6)\n",
    "    ax = pyplot.axes()\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    x_loss = list(range(len(losses)))\n",
    "    pyplot.plot(x_loss, losses)\n",
    "\n",
    "    if show:\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture\n",
    "- 33 inputs = 3 joint angles per leg, 4 legs, 2 DOF per joint. 4 touch sensors. 1 sine timestamp.\n",
    "- 28 outputs = *same as above, except just the joint angles*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitModel(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(GaitModel, self).__init__()\n",
    "\n",
    "\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU(), nn.BatchNorm1d(nl))\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer does not include an activation function.\n",
    "        # See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        tanh = torch.nn.Tanh()\n",
    "        \n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer] + [tanh]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now deprecated in favor of variable length NN (above)\n",
    "class PytorchBrain(torch.nn.Module):\n",
    "    _id = 0 # Global genome identifier\n",
    "    _num_inputs = 33\n",
    "    _num_outputs = 28\n",
    "\n",
    "    # @classmethod\n",
    "    # def __get_new_id(cls):\n",
    "    #     cls._id += 1\n",
    "    #     return cls._id\n",
    "\n",
    "    @classmethod\n",
    "    def get_num_outputs(cls):\n",
    "        return cls._num_outputs\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.h1 = torch.nn.Linear(PytorchBrain._num_inputs, 12)\n",
    "        self.relu = torch.nn.ReLU() #output is same shape as input\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(12)\n",
    "        self.h2 = torch.nn.Linear(12, 12)\n",
    "        self.h3 = torch.nn.Linear(12, 12)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = torch.nn.Linear(12, PytorchBrain._num_outputs)\n",
    "        \n",
    "        # Define sigmoid activation\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        # TODO: collapse all x = into \n",
    "        # TODO: either sigmoid OR Relu\n",
    "        x = self.h1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        # x = self.sigmoid(x)\n",
    "\n",
    "        x = self.h2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        # x = self.sigmoid(x)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        # x = self.sigmoid(x)\n",
    "\n",
    "        # TODO: linear then tanh might be better?\n",
    "        x = self.tanh(x) #caps output -1 to 1\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dataset, test_dataset, epochs=4, layer_sizes=[33, 31, 30, 28]):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 33\n",
    "    \n",
    "    data_loader_train = DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_test = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    pytorch_model = GaitModel(layer_sizes)\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss = train(model=pytorch_model, loader=data_loader_train, optimizer=optimizer, loss_fn=loss_fn, epochs=epochs)\n",
    "    \n",
    "\n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=pytorch_model, loader=data_loader_test)\n",
    "\n",
    "    return loss, y_predict, pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| train_dataset.length: 76\n",
      "ic| test_dataset.length: 20\n",
      "ic| 'Epoch: ', e+1: 1\n",
      "ic| 'Batches: ', batch_index: 3\n",
      "ic| 'Epoch: ', e+1: 51\n",
      "ic| 'Batches: ', batch_index: 153\n",
      "ic| 'Epoch: ', e+1: 101\n",
      "ic| 'Batches: ', batch_index: 303\n",
      "ic| 'Epoch: ', e+1: 151\n",
      "ic| 'Batches: ', batch_index: 453\n",
      "ic| 'Epoch: ', e+1: 201\n",
      "ic| 'Batches: ', batch_index: 603\n",
      "ic| 'Epoch: ', e+1: 251\n",
      "ic| 'Batches: ', batch_index: 753\n",
      "ic| 'Epoch: ', e+1: 301\n",
      "ic| 'Batches: ', batch_index: 903\n",
      "ic| 'Epoch: ', e+1: 351\n",
      "ic| 'Batches: ', batch_index: 1053\n",
      "ic| 'Final loss:', sum(losses[-100:])/100: 0.2750245717167854\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sqrt(losses[-1]): 0.5358247410345869\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ic(train_dataset.length)\n",
    "ic(test_dataset.length)\n",
    "\n",
    "losses, y_predict, model_to_save = run(train_dataset=train_dataset, test_dataset=test_dataset, epochs=400)\n",
    "torch.save(model_to_save, 'model.pt')\n",
    "\n",
    "ic(\"Final loss:\", sum(losses[-100:])/100)\n",
    "plot_loss(losses)\n",
    "ic(sqrt(losses[-1]))\n",
    "\n",
    "# Transpose the matrices so we can plot.\n",
    "# y_test = test_dataset.y_data.transpose()\n",
    "# y_predict = y_predict.transpose()\n",
    "y_test = np.transpose(test_dataset.y_data)\n",
    "y_predict = np.transpose(y_predict)\n",
    "\n",
    "\n",
    "# for yt, yp in zip(y_test, y_predict):\n",
    "#   fig2 = pyplot.figure(dpi=300)\n",
    "#   fig2.set_size_inches(8,6)\n",
    "#   ic(test_dataset.x_data.shape)\n",
    "#   ic(yt.shape)\n",
    "#   ic(yp.shape)\n",
    "#   pyplot.scatter(test_dataset.x_data, yt, marker='o', s=0.2)\n",
    "#   ic('second scatter')\n",
    "#   pyplot.scatter(test_dataset.x_data, yp, marker='o', s=0.3)\n",
    "#   pyplot.text(-9, 0.44, \"- Prediction\", color=\"orange\", fontsize=8)\n",
    "#   pyplot.text(-9, 0.48, \"- Sine (with noise)\", color=\"blue\", fontsize=8)\n",
    "#   pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2775866985321045,\n",
       " 1.2531042098999023,\n",
       " 1.2815170288085938,\n",
       " 1.2323991060256958,\n",
       " 1.211042881011963,\n",
       " 1.2442737817764282,\n",
       " 1.178427815437317,\n",
       " 1.1967875957489014,\n",
       " 1.1902172565460205,\n",
       " 1.1767165660858154,\n",
       " 1.1507971286773682,\n",
       " 1.1279467344284058,\n",
       " 1.1477597951889038,\n",
       " 1.1251152753829956,\n",
       " 1.1404474973678589,\n",
       " 1.1155198812484741,\n",
       " 1.104440689086914,\n",
       " 1.1329171657562256,\n",
       " 1.0974851846694946,\n",
       " 1.092179775238037,\n",
       " 1.1056679487228394,\n",
       " 1.0833462476730347,\n",
       " 1.0723727941513062,\n",
       " 1.0834715366363525,\n",
       " 1.0745505094528198,\n",
       " 1.0586943626403809,\n",
       " 1.0731661319732666,\n",
       " 1.0434117317199707,\n",
       " 1.0601402521133423,\n",
       " 1.0781490802764893,\n",
       " 1.045859932899475,\n",
       " 1.0263744592666626,\n",
       " 1.0390844345092773,\n",
       " 1.0270963907241821,\n",
       " 1.01631498336792,\n",
       " 1.0628658533096313,\n",
       " 1.011857032775879,\n",
       " 1.0017074346542358,\n",
       " 1.0294679403305054,\n",
       " 0.9940140247344971,\n",
       " 0.9882129430770874,\n",
       " 1.0180071592330933,\n",
       " 0.9766150116920471,\n",
       " 0.9842116236686707,\n",
       " 0.981016218662262,\n",
       " 0.9834722876548767,\n",
       " 0.9445496797561646,\n",
       " 1.0290979146957397,\n",
       " 0.951500654220581,\n",
       " 0.9684721231460571,\n",
       " 0.9732866287231445,\n",
       " 0.9579969644546509,\n",
       " 0.9467020630836487,\n",
       " 0.9397129416465759,\n",
       " 0.9270246624946594,\n",
       " 0.9279122352600098,\n",
       " 0.9581535458564758,\n",
       " 0.9386547803878784,\n",
       " 0.9072880744934082,\n",
       " 0.920853853225708,\n",
       " 0.8954278826713562,\n",
       " 0.926366925239563,\n",
       " 0.9297898411750793,\n",
       " 0.9130359888076782,\n",
       " 0.8734574913978577,\n",
       " 0.9133989810943604,\n",
       " 0.8868064284324646,\n",
       " 0.8790535926818848,\n",
       " 0.8625923991203308,\n",
       " 0.8744217753410339,\n",
       " 0.8669788837432861,\n",
       " 0.9108890295028687,\n",
       " 0.8453210592269897,\n",
       " 0.8721507787704468,\n",
       " 0.8361537456512451,\n",
       " 0.8624712228775024,\n",
       " 0.8395429253578186,\n",
       " 0.8411685228347778,\n",
       " 0.8261356353759766,\n",
       " 0.8386737704277039,\n",
       " 0.8488175868988037,\n",
       " 0.8192328810691833,\n",
       " 0.805801272392273,\n",
       " 0.8311130404472351,\n",
       " 0.8113924860954285,\n",
       " 0.7904233336448669,\n",
       " 0.8342831134796143,\n",
       " 0.7840309739112854,\n",
       " 0.7877767086029053,\n",
       " 0.7909061908721924,\n",
       " 0.7687931060791016,\n",
       " 0.772872805595398,\n",
       " 0.7782313823699951,\n",
       " 0.7525928616523743,\n",
       " 0.7479154467582703,\n",
       " 0.8668011426925659,\n",
       " 0.7561529874801636,\n",
       " 0.7460137009620667,\n",
       " 0.7457444071769714,\n",
       " 0.7429432272911072,\n",
       " 0.7227808833122253,\n",
       " 0.7362878322601318,\n",
       " 0.7176185250282288,\n",
       " 0.718903124332428,\n",
       " 0.7317162156105042,\n",
       " 0.7084311246871948,\n",
       " 0.705664873123169,\n",
       " 0.6816793084144592,\n",
       " 0.7013474106788635,\n",
       " 0.6874212622642517,\n",
       " 0.7241626381874084,\n",
       " 0.6819573640823364,\n",
       " 0.6877238154411316,\n",
       " 0.7071148753166199,\n",
       " 0.6640942096710205,\n",
       " 0.6487906575202942,\n",
       " 0.6766846179962158,\n",
       " 0.6602857112884521,\n",
       " 0.6436401605606079,\n",
       " 0.6651597619056702,\n",
       " 0.6363770961761475,\n",
       " 0.6280913949012756,\n",
       " 0.6266549825668335,\n",
       " 0.6211755871772766,\n",
       " 0.6153202652931213,\n",
       " 0.7202202677726746,\n",
       " 0.6252460479736328,\n",
       " 0.6093790531158447,\n",
       " 0.5895131230354309,\n",
       " 0.6073497533798218,\n",
       " 0.5724548697471619,\n",
       " 0.6252874135971069,\n",
       " 0.5908874273300171,\n",
       " 0.5756281614303589,\n",
       " 0.5685213208198547,\n",
       " 0.5516694188117981,\n",
       " 0.5882408022880554,\n",
       " 0.5543097853660583,\n",
       " 0.5599390864372253,\n",
       " 0.5562219023704529,\n",
       " 0.6117620468139648,\n",
       " 0.5534057021141052,\n",
       " 0.5341692566871643,\n",
       " 0.5806043744087219,\n",
       " 0.5185526609420776,\n",
       " 0.5361537337303162,\n",
       " 0.5577652454376221,\n",
       " 0.5338480472564697,\n",
       " 0.5091637969017029,\n",
       " 0.5122339725494385,\n",
       " 0.5066448450088501,\n",
       " 0.507174015045166,\n",
       " 0.5636573433876038,\n",
       " 0.5150063037872314,\n",
       " 0.49575918912887573,\n",
       " 0.5126808881759644,\n",
       " 0.5013574361801147,\n",
       " 0.5123363733291626,\n",
       " 0.48006340861320496,\n",
       " 0.49317288398742676,\n",
       " 0.4750908613204956,\n",
       " 0.4875922203063965,\n",
       " 0.48579517006874084,\n",
       " 0.48398715257644653,\n",
       " 0.4713464379310608,\n",
       " 0.4776754081249237,\n",
       " 0.4779515862464905,\n",
       " 0.4473114609718323,\n",
       " 0.4643709063529968,\n",
       " 0.4456644654273987,\n",
       " 0.4577430784702301,\n",
       " 0.4560104012489319,\n",
       " 0.46583291888237,\n",
       " 0.4668084383010864,\n",
       " 0.45241811871528625,\n",
       " 0.4434070885181427,\n",
       " 0.46847066283226013,\n",
       " 0.44049307703971863,\n",
       " 0.425820916891098,\n",
       " 0.47693225741386414,\n",
       " 0.44373220205307007,\n",
       " 0.4209313988685608,\n",
       " 0.48841530084609985,\n",
       " 0.43256765604019165,\n",
       " 0.42274340987205505,\n",
       " 0.4207523465156555,\n",
       " 0.41964012384414673,\n",
       " 0.4137434661388397,\n",
       " 0.5007652640342712,\n",
       " 0.43744662404060364,\n",
       " 0.4245056211948395,\n",
       " 0.4213079810142517,\n",
       " 0.42144593596458435,\n",
       " 0.414438396692276,\n",
       " 0.43650564551353455,\n",
       " 0.3959697186946869,\n",
       " 0.4226229786872864,\n",
       " 0.4239160716533661,\n",
       " 0.40938684344291687,\n",
       " 0.40691903233528137,\n",
       " 0.4163583517074585,\n",
       " 0.40263161063194275,\n",
       " 0.388025164604187,\n",
       " 0.4635540544986725,\n",
       " 0.382053405046463,\n",
       " 0.4115891754627228,\n",
       " 0.399282306432724,\n",
       " 0.403602659702301,\n",
       " 0.38263681530952454,\n",
       " 0.40614908933639526,\n",
       " 0.3753321170806885,\n",
       " 0.3915462791919708,\n",
       " 0.4214111566543579,\n",
       " 0.38146647810935974,\n",
       " 0.3714754581451416,\n",
       " 0.43299585580825806,\n",
       " 0.39451494812965393,\n",
       " 0.3792421817779541,\n",
       " 0.3904082179069519,\n",
       " 0.3608328700065613,\n",
       " 0.4069572985172272,\n",
       " 0.3900619149208069,\n",
       " 0.37391582131385803,\n",
       " 0.3882836401462555,\n",
       " 0.3853960335254669,\n",
       " 0.3819299042224884,\n",
       " 0.3773748278617859,\n",
       " 0.3490849435329437,\n",
       " 0.3774021565914154,\n",
       " 0.36952266097068787,\n",
       " 0.3957045078277588,\n",
       " 0.3657281994819641,\n",
       " 0.3750806152820587,\n",
       " 0.3792744278907776,\n",
       " 0.3720436990261078,\n",
       " 0.3613535165786743,\n",
       " 0.3696560263633728,\n",
       " 0.36196210980415344,\n",
       " 0.3616880476474762,\n",
       " 0.375628262758255,\n",
       " 0.3703882396221161,\n",
       " 0.3420884907245636,\n",
       " 0.3549876809120178,\n",
       " 0.3551441431045532,\n",
       " 0.3525822162628174,\n",
       " 0.35581687092781067,\n",
       " 0.3564996123313904,\n",
       " 0.3566437363624573,\n",
       " 0.3416541516780853,\n",
       " 0.3433651626110077,\n",
       " 0.3548971712589264,\n",
       " 0.3634391129016876,\n",
       " 0.33248698711395264,\n",
       " 0.3471832573413849,\n",
       " 0.34936508536338806,\n",
       " 0.3394240438938141,\n",
       " 0.34279200434684753,\n",
       " 0.3302256166934967,\n",
       " 0.33787113428115845,\n",
       " 0.33223798871040344,\n",
       " 0.32568350434303284,\n",
       " 0.3405183255672455,\n",
       " 0.3308487832546234,\n",
       " 0.3238968551158905,\n",
       " 0.33249107003211975,\n",
       " 0.32604119181632996,\n",
       " 0.3229754865169525,\n",
       " 0.32797953486442566,\n",
       " 0.339336633682251,\n",
       " 0.30521467328071594,\n",
       " 0.3242601752281189,\n",
       " 0.3192344903945923,\n",
       " 0.3259778916835785,\n",
       " 0.3214649558067322,\n",
       " 0.32297343015670776,\n",
       " 0.3093877136707306,\n",
       " 0.3161911964416504,\n",
       " 0.31874796748161316,\n",
       " 0.3153925836086273,\n",
       " 0.31270378828048706,\n",
       " 0.31280362606048584,\n",
       " 0.3270026743412018,\n",
       " 0.3117979168891907,\n",
       " 0.3125658631324768,\n",
       " 0.31960079073905945,\n",
       " 0.3140503466129303,\n",
       " 0.30561205744743347,\n",
       " 0.31913360953330994,\n",
       " 0.3052876889705658,\n",
       " 0.3193241059780121,\n",
       " 0.31417346000671387,\n",
       " 0.31559452414512634,\n",
       " 0.3043779730796814,\n",
       " 0.3015630543231964,\n",
       " 0.3069557547569275,\n",
       " 0.30695733428001404,\n",
       " 0.3047356605529785,\n",
       " 0.2991519868373871,\n",
       " 0.30579492449760437,\n",
       " 0.3254595994949341,\n",
       " 0.30337899923324585,\n",
       " 0.3081640601158142,\n",
       " 0.30739396810531616,\n",
       " 0.305181086063385,\n",
       " 0.3050655424594879,\n",
       " 0.3046089708805084,\n",
       " 0.30428746342658997,\n",
       " 0.3050648272037506,\n",
       " 0.290310800075531,\n",
       " 0.299271821975708,\n",
       " 0.30417096614837646,\n",
       " 0.2992071807384491,\n",
       " 0.2941623032093048,\n",
       " 0.30602002143859863,\n",
       " 0.30316469073295593,\n",
       " 0.3044641315937042,\n",
       " 0.29312440752983093,\n",
       " 0.3108055293560028,\n",
       " 0.29739096760749817,\n",
       " 0.30273887515068054,\n",
       " 0.2926202118396759,\n",
       " 0.29630741477012634,\n",
       " 0.30217206478118896,\n",
       " 0.29194849729537964,\n",
       " 0.3016104996204376,\n",
       " 0.29632896184921265,\n",
       " 0.2970997393131256,\n",
       " 0.29642385244369507,\n",
       " 0.29891547560691833,\n",
       " 0.28936684131622314,\n",
       " 0.2931113839149475,\n",
       " 0.3019104599952698,\n",
       " 0.28551971912384033,\n",
       " 0.2913696765899658,\n",
       " 0.2996973395347595,\n",
       " 0.2956281006336212,\n",
       " 0.3006227910518646,\n",
       " 0.29105180501937866,\n",
       " 0.2916281819343567,\n",
       " 0.2890494167804718,\n",
       " 0.30298614501953125,\n",
       " 0.2910131514072418,\n",
       " 0.2901579439640045,\n",
       " 0.29961588978767395,\n",
       " 0.29049304127693176,\n",
       " 0.2954123318195343,\n",
       " 0.2891292870044708,\n",
       " 0.3026883602142334,\n",
       " 0.29432564973831177,\n",
       " 0.29245176911354065,\n",
       " 0.29622042179107666,\n",
       " 0.28935107588768005,\n",
       " 0.29546359181404114,\n",
       " 0.29304417967796326,\n",
       " 0.29471856355667114,\n",
       " 0.29531097412109375,\n",
       " 0.28212156891822815,\n",
       " 0.28749969601631165,\n",
       " 0.29867780208587646,\n",
       " 0.28899502754211426,\n",
       " 0.2951142489910126,\n",
       " 0.29105743765830994,\n",
       " 0.2984035909175873,\n",
       " 0.2957105040550232,\n",
       " 0.28764742612838745,\n",
       " 0.29773983359336853,\n",
       " 0.2956940829753876,\n",
       " 0.28694722056388855,\n",
       " 0.290971577167511,\n",
       " 0.29265642166137695,\n",
       " 0.28609415888786316,\n",
       " 0.29744377732276917,\n",
       " 0.2861836850643158,\n",
       " 0.2925609350204468,\n",
       " 0.28919750452041626,\n",
       " 0.28578290343284607,\n",
       " 0.2897408902645111,\n",
       " 0.29962608218193054,\n",
       " 0.28911226987838745,\n",
       " 0.28719061613082886,\n",
       " 0.30069857835769653,\n",
       " 0.2870096266269684,\n",
       " 0.2883760631084442,\n",
       " 0.29872116446495056,\n",
       " 0.28925222158432007,\n",
       " 0.28971952199935913,\n",
       " 0.2896689474582672,\n",
       " 0.2809361517429352,\n",
       " 0.2935214936733246,\n",
       " 0.29589352011680603,\n",
       " 0.2894747257232666,\n",
       " 0.28636011481285095,\n",
       " 0.29753971099853516,\n",
       " 0.28428158164024353,\n",
       " 0.28658998012542725,\n",
       " 0.304544061422348,\n",
       " 0.28712281584739685,\n",
       " 0.28847694396972656,\n",
       " 0.29909196496009827,\n",
       " 0.2924844026565552,\n",
       " 0.28421053290367126,\n",
       " 0.29002994298934937,\n",
       " 0.2793523967266083,\n",
       " 0.29070010781288147,\n",
       " 0.3174390196800232,\n",
       " 0.2834549844264984,\n",
       " 0.28663793206214905,\n",
       " 0.29651132225990295,\n",
       " 0.28456324338912964,\n",
       " 0.28792306780815125,\n",
       " 0.28991779685020447,\n",
       " 0.28716880083084106,\n",
       " 0.2880786061286926,\n",
       " 0.28169286251068115,\n",
       " 0.2870939373970032,\n",
       " 0.2816351354122162,\n",
       " 0.29108351469039917,\n",
       " 0.28354761004447937,\n",
       " 0.287840873003006,\n",
       " 0.2835533618927002,\n",
       " 0.2852896749973297,\n",
       " 0.28536710143089294,\n",
       " 0.2903272211551666,\n",
       " 0.2803005278110504,\n",
       " 0.2853306829929352,\n",
       " 0.30153530836105347,\n",
       " 0.2848477065563202,\n",
       " 0.2897854745388031,\n",
       " 0.2867153286933899,\n",
       " 0.28124362230300903,\n",
       " 0.2853471040725708,\n",
       " 0.2949140667915344,\n",
       " 0.28373482823371887,\n",
       " 0.2838755249977112,\n",
       " 0.29315605759620667,\n",
       " 0.2883709669113159,\n",
       " 0.2797963619232178,\n",
       " 0.2868901789188385,\n",
       " 0.28390437364578247,\n",
       " 0.28678399324417114,\n",
       " 0.27845847606658936,\n",
       " 0.28380024433135986,\n",
       " 0.28259021043777466,\n",
       " 0.29380014538764954,\n",
       " 0.2864871323108673,\n",
       " 0.28245142102241516,\n",
       " 0.28000351786613464,\n",
       " 0.2797712981700897,\n",
       " 0.28330567479133606,\n",
       " 0.2958473265171051,\n",
       " 0.28754568099975586,\n",
       " 0.2780297100543976,\n",
       " 0.2997868061065674,\n",
       " 0.2790364623069763,\n",
       " 0.28505322337150574,\n",
       " 0.29823392629623413,\n",
       " 0.28244009613990784,\n",
       " 0.2826545238494873,\n",
       " 0.2947733700275421,\n",
       " 0.28323784470558167,\n",
       " 0.28781676292419434,\n",
       " 0.2771639823913574,\n",
       " 0.28323888778686523,\n",
       " 0.28484222292900085,\n",
       " 0.288772851228714,\n",
       " 0.27742448449134827,\n",
       " 0.28817421197891235,\n",
       " 0.2829457223415375,\n",
       " 0.2784142792224884,\n",
       " 0.2888123691082001,\n",
       " 0.2844080328941345,\n",
       " 0.28106608986854553,\n",
       " 0.2805284857749939,\n",
       " 0.294986754655838,\n",
       " 0.2860659956932068,\n",
       " 0.27508607506752014,\n",
       " 0.299257755279541,\n",
       " 0.28271380066871643,\n",
       " 0.28030744194984436,\n",
       " 0.28939753770828247,\n",
       " 0.2834317088127136,\n",
       " 0.27964189648628235,\n",
       " 0.2928715646266937,\n",
       " 0.28316736221313477,\n",
       " 0.28063416481018066,\n",
       " 0.284201979637146,\n",
       " 0.28206390142440796,\n",
       " 0.28034839034080505,\n",
       " 0.29213571548461914,\n",
       " 0.27952301502227783,\n",
       " 0.28167054057121277,\n",
       " 0.2919080853462219,\n",
       " 0.2787918448448181,\n",
       " 0.2835628092288971,\n",
       " 0.28801894187927246,\n",
       " 0.28452369570732117,\n",
       " 0.27485644817352295,\n",
       " 0.29758405685424805,\n",
       " 0.28099241852760315,\n",
       " 0.2775145173072815,\n",
       " 0.29808309674263,\n",
       " 0.2781215310096741,\n",
       " 0.28439977765083313,\n",
       " 0.2850877046585083,\n",
       " 0.28260350227355957,\n",
       " 0.28218528628349304,\n",
       " 0.2765769958496094,\n",
       " 0.28401145339012146,\n",
       " 0.28216156363487244,\n",
       " 0.2706005871295929,\n",
       " 0.2779979407787323,\n",
       " 0.28345879912376404,\n",
       " 0.2862953841686249,\n",
       " 0.2816964387893677,\n",
       " 0.2809670567512512,\n",
       " 0.27666258811950684,\n",
       " 0.28006356954574585,\n",
       " 0.28201860189437866,\n",
       " 0.2846822738647461,\n",
       " 0.2812420129776001,\n",
       " 0.2799373269081116,\n",
       " 0.2835729122161865,\n",
       " 0.28859588503837585,\n",
       " 0.27674806118011475,\n",
       " 0.27673399448394775,\n",
       " 0.28731951117515564,\n",
       " 0.2755756378173828,\n",
       " 0.28153693675994873,\n",
       " 0.27683934569358826,\n",
       " 0.2859817445278168,\n",
       " 0.27746662497520447,\n",
       " 0.2832513451576233,\n",
       " 0.27764853835105896,\n",
       " 0.2820948362350464,\n",
       " 0.27886199951171875,\n",
       " 0.28050947189331055,\n",
       " 0.28807544708251953,\n",
       " 0.2852608561515808,\n",
       " 0.27692708373069763,\n",
       " 0.28101518750190735,\n",
       " 0.28023645281791687,\n",
       " 0.2760940194129944,\n",
       " 0.2949782609939575,\n",
       " 0.28448590636253357,\n",
       " 0.2735295593738556,\n",
       " 0.2887554168701172,\n",
       " 0.27990251779556274,\n",
       " 0.28059735894203186,\n",
       " 0.2748588025569916,\n",
       " 0.28250086307525635,\n",
       " 0.2746729850769043,\n",
       " 0.2868332862854004,\n",
       " 0.2767332196235657,\n",
       " 0.27970439195632935,\n",
       " 0.29004979133605957,\n",
       " 0.2845592200756073,\n",
       " 0.2782321572303772,\n",
       " 0.27027109265327454,\n",
       " 0.27569344639778137,\n",
       " 0.28192800283432007,\n",
       " 0.290527880191803,\n",
       " 0.2833448350429535,\n",
       " 0.2799563705921173,\n",
       " 0.28029289841651917,\n",
       " 0.2806617319583893,\n",
       " 0.2803064286708832,\n",
       " 0.2783891260623932,\n",
       " 0.27804556488990784,\n",
       " 0.2794415056705475,\n",
       " 0.28988218307495117,\n",
       " 0.27979448437690735,\n",
       " 0.2789582312107086,\n",
       " 0.2800147831439972,\n",
       " 0.2787304222583771,\n",
       " 0.28169092535972595,\n",
       " 0.27620258927345276,\n",
       " 0.28165313601493835,\n",
       " 0.27670368552207947,\n",
       " 0.2814878225326538,\n",
       " 0.2829340398311615,\n",
       " 0.27238088846206665,\n",
       " 0.2896198630332947,\n",
       " 0.27937665581703186,\n",
       " 0.2820189893245697,\n",
       " 0.28878021240234375,\n",
       " 0.27846673130989075,\n",
       " 0.2757350504398346,\n",
       " 0.2907485365867615,\n",
       " 0.2759914696216583,\n",
       " 0.2772650122642517,\n",
       " 0.3108724057674408,\n",
       " 0.2747546136379242,\n",
       " 0.2808625400066376,\n",
       " 0.29308074712753296,\n",
       " 0.2792379856109619,\n",
       " 0.2801705598831177,\n",
       " 0.27440646290779114,\n",
       " 0.2739429771900177,\n",
       " 0.28088968992233276,\n",
       " 0.288409560918808,\n",
       " 0.2751748859882355,\n",
       " 0.28430458903312683,\n",
       " 0.2729998826980591,\n",
       " 0.2783597409725189,\n",
       " 0.2800827622413635,\n",
       " 0.2723679840564728,\n",
       " 0.2814619839191437,\n",
       " 0.2774154245853424,\n",
       " 0.27345937490463257,\n",
       " 0.2784663140773773,\n",
       " 0.2832029461860657,\n",
       " 0.27514949440956116,\n",
       " 0.28452038764953613,\n",
       " 0.2750714123249054,\n",
       " 0.2824305593967438,\n",
       " 0.27264460921287537,\n",
       " 0.2896503508090973,\n",
       " 0.2778930068016052,\n",
       " 0.277915894985199,\n",
       " 0.28152996301651,\n",
       " 0.2743840217590332,\n",
       " 0.27578169107437134,\n",
       " 0.279005229473114,\n",
       " 0.28815752267837524,\n",
       " 0.28120461106300354,\n",
       " 0.2788434326648712,\n",
       " 0.2663663923740387,\n",
       " 0.2781893014907837,\n",
       " 0.2811892032623291,\n",
       " 0.2736102342605591,\n",
       " 0.2735077738761902,\n",
       " 0.28139618039131165,\n",
       " 0.28180375695228577,\n",
       " 0.2748299539089203,\n",
       " 0.2887243926525116,\n",
       " 0.26515138149261475,\n",
       " 0.28205302357673645,\n",
       " 0.27911171317100525,\n",
       " 0.2718721628189087,\n",
       " 0.28157275915145874,\n",
       " 0.2741773724555969,\n",
       " 0.2847674489021301,\n",
       " 0.2755110561847687,\n",
       " 0.2803460657596588,\n",
       " 0.2843652069568634,\n",
       " 0.2805737555027008,\n",
       " 0.27071669697761536,\n",
       " 0.2950122356414795,\n",
       " 0.2793337106704712,\n",
       " 0.2757514417171478,\n",
       " 0.2890952229499817,\n",
       " 0.27424830198287964,\n",
       " 0.2804225981235504,\n",
       " 0.2791832387447357,\n",
       " 0.2778734564781189,\n",
       " 0.2790203392505646,\n",
       " 0.2780776619911194,\n",
       " 0.27523425221443176,\n",
       " 0.28250619769096375,\n",
       " 0.2745770812034607,\n",
       " 0.27832067012786865,\n",
       " 0.2745392918586731,\n",
       " 0.28753432631492615,\n",
       " 0.27803337574005127,\n",
       " 0.2769303321838379,\n",
       " 0.2858862280845642,\n",
       " 0.27861180901527405,\n",
       " 0.27760377526283264,\n",
       " 0.27586254477500916,\n",
       " 0.2741122245788574,\n",
       " 0.28235334157943726,\n",
       " 0.2736169397830963,\n",
       " 0.284354031085968,\n",
       " 0.27329105138778687,\n",
       " 0.2718101441860199,\n",
       " 0.2779046893119812,\n",
       " 0.2765306532382965,\n",
       " 0.27741894125938416,\n",
       " 0.27357402443885803,\n",
       " 0.2793683707714081,\n",
       " 0.28373605012893677,\n",
       " 0.28425365686416626,\n",
       " 0.27583321928977966,\n",
       " 0.2697485685348511,\n",
       " 0.2810100317001343,\n",
       " 0.2742792069911957,\n",
       " 0.2817607522010803,\n",
       " 0.27645188570022583,\n",
       " 0.2772117257118225,\n",
       " 0.28010258078575134,\n",
       " 0.2776595652103424,\n",
       " 0.2755179703235626,\n",
       " 0.2828976511955261,\n",
       " 0.27598729729652405,\n",
       " 0.2771841883659363,\n",
       " 0.28173261880874634,\n",
       " 0.2708626985549927,\n",
       " 0.28389495611190796,\n",
       " 0.27568134665489197,\n",
       " 0.2758695185184479,\n",
       " 0.27630653977394104,\n",
       " 0.2871118187904358,\n",
       " 0.2780776917934418,\n",
       " 0.2780624032020569,\n",
       " 0.2723708152770996,\n",
       " 0.2823236584663391,\n",
       " 0.2724541127681732,\n",
       " 0.2745385766029358,\n",
       " 0.27330145239830017,\n",
       " 0.2803730368614197,\n",
       " 0.2804955542087555,\n",
       " 0.2740139663219452,\n",
       " 0.27626657485961914,\n",
       " 0.2868487536907196,\n",
       " 0.27577486634254456,\n",
       " 0.2767349183559418,\n",
       " 0.2841057777404785,\n",
       " 0.27952539920806885,\n",
       " 0.2787453234195709,\n",
       " 0.2645869255065918,\n",
       " 0.2785003185272217,\n",
       " 0.2764115333557129,\n",
       " 0.27855736017227173,\n",
       " 0.2763037383556366,\n",
       " 0.2767084240913391,\n",
       " 0.28503182530403137,\n",
       " 0.2769441306591034,\n",
       " 0.27497434616088867,\n",
       " 0.28414300084114075,\n",
       " 0.2855653762817383,\n",
       " 0.26962512731552124,\n",
       " 0.27215754985809326,\n",
       " 0.2749590277671814,\n",
       " 0.272882342338562,\n",
       " 0.31107503175735474,\n",
       " 0.2767587900161743,\n",
       " 0.27525973320007324,\n",
       " 0.2796424627304077,\n",
       " 0.2745545506477356,\n",
       " 0.27769234776496887,\n",
       " 0.28233879804611206,\n",
       " 0.2786082923412323,\n",
       " 0.27981463074684143,\n",
       " 0.26741912961006165,\n",
       " 0.2739229202270508,\n",
       " 0.2778579592704773,\n",
       " 0.28229793906211853,\n",
       " 0.2803451716899872,\n",
       " 0.2735462188720703,\n",
       " 0.2703264653682709,\n",
       " 0.27960699796676636,\n",
       " 0.2712867558002472,\n",
       " 0.2801271080970764,\n",
       " 0.27424338459968567,\n",
       " 0.2782594561576843,\n",
       " 0.277689665555954,\n",
       " 0.2745458781719208,\n",
       " 0.27539151906967163,\n",
       " 0.2844197154045105,\n",
       " 0.2769205868244171,\n",
       " 0.27690649032592773,\n",
       " 0.269998699426651,\n",
       " 0.2733280658721924,\n",
       " 0.2762468457221985,\n",
       " 0.29259124398231506,\n",
       " 0.2723439335823059,\n",
       " 0.2776763141155243,\n",
       " 0.28401803970336914,\n",
       " 0.27685460448265076,\n",
       " 0.27926838397979736,\n",
       " 0.268479585647583,\n",
       " 0.2762216031551361,\n",
       " 0.27490976452827454,\n",
       " 0.28015413880348206,\n",
       " 0.2758510410785675,\n",
       " 0.2792108654975891,\n",
       " 0.2692316770553589,\n",
       " 0.2734847962856293,\n",
       " 0.28152674436569214,\n",
       " 0.27006644010543823,\n",
       " 0.27501124143600464,\n",
       " 0.2749635875225067,\n",
       " 0.2808515131473541,\n",
       " 0.2756449282169342,\n",
       " 0.27671894431114197,\n",
       " 0.280184805393219,\n",
       " 0.27652493119239807,\n",
       " 0.2748151123523712,\n",
       " 0.278957337141037,\n",
       " 0.27349424362182617,\n",
       " 0.2761439085006714,\n",
       " 0.2855501174926758,\n",
       " 0.2683873474597931,\n",
       " 0.2834959924221039,\n",
       " 0.27690473198890686,\n",
       " 0.28083598613739014,\n",
       " 0.2689237594604492,\n",
       " 0.283413827419281,\n",
       " 0.2796871066093445,\n",
       " 0.2739601731300354,\n",
       " 0.2660429775714874,\n",
       " 0.27824169397354126,\n",
       " 0.27194055914878845,\n",
       " 0.2785446345806122,\n",
       " 0.27569714188575745,\n",
       " 0.2789282500743866,\n",
       " 0.27094486355781555,\n",
       " 0.2738790512084961,\n",
       " 0.2779836654663086,\n",
       " 0.27445533871650696,\n",
       " 0.2745641767978668,\n",
       " 0.27682939171791077,\n",
       " 0.2767311632633209,\n",
       " 0.27506640553474426,\n",
       " 0.2759642004966736,\n",
       " 0.27852359414100647,\n",
       " 0.2823798954486847,\n",
       " 0.2685386836528778,\n",
       " 0.27437156438827515,\n",
       " 0.2751401662826538,\n",
       " 0.2776181101799011,\n",
       " 0.26693713665008545,\n",
       " 0.2785710394382477,\n",
       " 0.270151287317276,\n",
       " 0.29695433378219604,\n",
       " 0.28041818737983704,\n",
       " 0.2739467918872833,\n",
       " 0.27317503094673157,\n",
       " 0.2741212248802185,\n",
       " 0.27531394362449646,\n",
       " 0.28232908248901367,\n",
       " 0.2720976173877716,\n",
       " 0.2780897319316864,\n",
       " 0.2775682210922241,\n",
       " 0.27267587184906006,\n",
       " 0.27827760577201843,\n",
       " 0.2749607563018799,\n",
       " 0.2779851257801056,\n",
       " 0.27319636940956116,\n",
       " 0.27465683221817017,\n",
       " 0.2763512134552002,\n",
       " 0.2733260691165924,\n",
       " 0.2819143533706665,\n",
       " 0.27739599347114563,\n",
       " 0.27388542890548706,\n",
       " 0.28376781940460205,\n",
       " 0.2799032926559448,\n",
       " 0.2739538848400116,\n",
       " 0.26718759536743164,\n",
       " 0.2705073058605194,\n",
       " 0.2783978581428528,\n",
       " 0.28761377930641174,\n",
       " 0.27236148715019226,\n",
       " 0.27554264664649963,\n",
       " 0.294580340385437,\n",
       " 0.27282971143722534,\n",
       " 0.28135010600090027,\n",
       " 0.26768577098846436,\n",
       " 0.27894648909568787,\n",
       " 0.273160457611084,\n",
       " 0.2714178264141083,\n",
       " 0.2699226438999176,\n",
       " 0.2832011878490448,\n",
       " 0.27014628052711487,\n",
       " 0.27535480260849,\n",
       " 0.27426719665527344,\n",
       " 0.28339236974716187,\n",
       " 0.28171253204345703,\n",
       " 0.272525817155838,\n",
       " 0.26693058013916016,\n",
       " 0.2729540765285492,\n",
       " 0.2763323187828064,\n",
       " 0.27988511323928833,\n",
       " 0.2697417140007019,\n",
       " 0.2774229049682617,\n",
       " 0.2896139621734619,\n",
       " 0.2734512984752655,\n",
       " 0.28122198581695557,\n",
       " 0.27611514925956726,\n",
       " 0.27181947231292725,\n",
       " 0.27822744846343994,\n",
       " 0.2813313901424408,\n",
       " 0.27508461475372314,\n",
       " 0.2754313051700592,\n",
       " 0.28241097927093506,\n",
       " 0.27518755197525024,\n",
       " 0.2764853835105896,\n",
       " 0.27457883954048157,\n",
       " 0.2729516327381134,\n",
       " 0.2747754752635956,\n",
       " 0.2826988995075226,\n",
       " 0.2759910225868225,\n",
       " 0.27722278237342834,\n",
       " 0.26276955008506775,\n",
       " 0.27332934737205505,\n",
       " 0.27820417284965515,\n",
       " 0.26862505078315735,\n",
       " 0.2705032527446747,\n",
       " 0.2784518301486969,\n",
       " 0.27923548221588135,\n",
       " 0.2778962552547455,\n",
       " 0.2715632915496826,\n",
       " 0.27964144945144653,\n",
       " 0.2764382064342499,\n",
       " 0.27223214507102966,\n",
       " 0.2798313498497009,\n",
       " 0.2721526622772217,\n",
       " 0.2766978442668915,\n",
       " 0.2754279375076294,\n",
       " 0.27226048707962036,\n",
       " 0.2768194079399109,\n",
       " 0.2804228365421295,\n",
       " 0.27215346693992615,\n",
       " 0.2772960960865021,\n",
       " 0.27399110794067383,\n",
       " 0.27678120136260986,\n",
       " 0.2747773230075836,\n",
       " 0.27156588435173035,\n",
       " 0.27507928013801575,\n",
       " 0.2748788297176361,\n",
       " 0.2842208445072174,\n",
       " 0.27341899275779724,\n",
       " 0.2738574147224426,\n",
       " 0.2840447425842285,\n",
       " 0.2681710124015808,\n",
       " 0.2826195955276489,\n",
       " 0.2762382924556732,\n",
       " 0.27316606044769287,\n",
       " 0.27550143003463745,\n",
       " 0.2837947607040405,\n",
       " 0.273006796836853,\n",
       " 0.27390262484550476,\n",
       " 0.29928305745124817,\n",
       " 0.2765384316444397,\n",
       " 0.2717072665691376,\n",
       " 0.2782174050807953,\n",
       " 0.2788577079772949,\n",
       " 0.2754068970680237,\n",
       " 0.2673066258430481,\n",
       " 0.28082630038261414,\n",
       " 0.26814401149749756,\n",
       " 0.2884279787540436,\n",
       " 0.27493801712989807,\n",
       " 0.2734425961971283,\n",
       " 0.2783603072166443,\n",
       " 0.26908591389656067,\n",
       " 0.2799127995967865,\n",
       " 0.2733229994773865,\n",
       " 0.27508869767189026,\n",
       " 0.26993313431739807,\n",
       " 0.29638898372650146,\n",
       " 0.2737579643726349,\n",
       " 0.27725672721862793,\n",
       " 0.27080297470092773,\n",
       " 0.2741919755935669,\n",
       " 0.27597370743751526,\n",
       " 0.2749905586242676,\n",
       " 0.27473559975624084,\n",
       " 0.2717811167240143,\n",
       " 0.2810989320278168,\n",
       " 0.27167510986328125,\n",
       " 0.27631017565727234,\n",
       " 0.27953171730041504,\n",
       " 0.2752250134944916,\n",
       " 0.2723378837108612,\n",
       " 0.27770036458969116,\n",
       " 0.27664393186569214,\n",
       " 0.27525854110717773,\n",
       " 0.2642935812473297,\n",
       " 0.2732647657394409,\n",
       " 0.26997673511505127,\n",
       " 0.29463788866996765,\n",
       " 0.2733725309371948,\n",
       " 0.2717253565788269,\n",
       " 0.29633504152297974,\n",
       " 0.2720038890838623,\n",
       " 0.2770962417125702,\n",
       " 0.27418553829193115,\n",
       " 0.2745725214481354,\n",
       " 0.27218419313430786,\n",
       " 0.2792131304740906,\n",
       " 0.2772003412246704,\n",
       " 0.27513331174850464,\n",
       " 0.26315417885780334,\n",
       " 0.2686188817024231,\n",
       " 0.2750416100025177,\n",
       " 0.29360419511795044,\n",
       " 0.26929354667663574,\n",
       " 0.27731576561927795,\n",
       " 0.2867336571216583,\n",
       " 0.2736523151397705,\n",
       " 0.27541428804397583,\n",
       " 0.27375635504722595,\n",
       " 0.26904645562171936,\n",
       " 0.2769569754600525,\n",
       " 0.28627705574035645,\n",
       " 0.2785657048225403,\n",
       " 0.2689962387084961,\n",
       " 0.2778516709804535,\n",
       " 0.2726331651210785,\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e96169c3d85a0b7abfdf8329564d63e23581ffb90e909b0554daf0689ed6e7b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
