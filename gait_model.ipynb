{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'USING pytorch VERSION: ', torch.__version__: '1.6.0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('USING pytorch VERSION: ', '1.6.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "ic(\"USING pytorch VERSION: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our data is in numpy format, which conveniently pytorch has a method for converting to their native format.\n",
    "\n",
    "The init function takes the path to the csv and creates a dataset out of it. I actually have three different options here. The dataset could be composed such that x is the 'timestamp' of the movement,the previous set of angles, or a tuple of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(x_data): 12\n",
      "ic| len(x_data): 24\n",
      "ic| len(x_data): 36\n",
      "ic| len(x_data): 48\n",
      "ic| len(x_data): 60\n",
      "ic| len(x_data): 72\n",
      "ic| len(x_data): 84\n",
      "ic| len(x_data): 96\n",
      "ic| len(x_data): 108\n",
      "ic| len(x_data): 120\n",
      "ic| len(x_data): 132\n",
      "ic| len(x_data): 144\n",
      "ic| len(x_data): 156\n",
      "ic| len(x_data): 168\n",
      "ic| len(x_data): 180\n",
      "ic| len(x_data): 192\n",
      "ic| len(x_data): 204\n",
      "ic| len(x_data): 216\n",
      "ic| len(x_data): 228\n",
      "ic| len(x_data): 240\n",
      "ic| len(x_data): 252\n",
      "ic| len(x_data): 264\n",
      "ic| len(x_data): 276\n",
      "ic| len(x_data): 288\n",
      "ic| len(x_data): 300\n",
      "ic| len(x_data): 312\n",
      "ic| len(x_data): 324\n",
      "ic| len(x_data): 336\n",
      "ic| len(x_data): 348\n",
      "ic| len(x_data): 360\n",
      "ic| len(x_data): 372\n",
      "ic| len(x_data): 384\n",
      "ic| len(x_data): 396\n",
      "ic| len(x_data): 408\n",
      "ic| len(x_data): 420\n",
      "ic| len(x_data): 432\n",
      "ic| len(x_data): 444\n",
      "ic| len(x_data): 456\n",
      "ic| len(x_data): 468\n",
      "ic| len(x_data): 480\n",
      "ic| len(x_data): 492\n",
      "ic| len(x_data): 504\n",
      "ic| len(x_data): 516\n",
      "ic| len(x_data): 528\n",
      "ic| len(x_data): 540\n",
      "ic| len(x_data): 552\n",
      "ic| len(x_data): 564\n",
      "ic| len(x_data): 576\n",
      "ic| len(x_data): 588\n",
      "ic| len(x_data): 600\n",
      "ic| len(x_data): 612\n",
      "ic| len(x_data): 624\n",
      "ic| len(x_data): 636\n",
      "ic| len(x_data): 648\n",
      "ic| len(x_data): 660\n",
      "ic| len(x_data): 672\n",
      "ic| len(x_data): 684\n",
      "ic| len(x_data): 696\n",
      "ic| len(x_data): 708\n",
      "ic| len(x_data): 720\n",
      "ic| len(x_data): 732\n",
      "ic| len(x_data): 744\n",
      "ic| len(x_data): 756\n",
      "ic| len(x_data): 768\n",
      "ic| len(x_data): 780\n",
      "ic| len(x_data): 792\n",
      "ic| len(x_data): 804\n",
      "ic| len(x_data): 816\n",
      "ic| len(x_data): 828\n",
      "ic| len(x_data): 840\n",
      "ic| len(x_data): 852\n",
      "ic| len(x_data): 864\n",
      "ic| len(x_data): 876\n",
      "ic| len(x_data): 888\n",
      "ic| len(x_data): 900\n",
      "ic| len(x_data): 912\n",
      "ic| len(x_data): 924\n",
      "ic| len(x_data): 936\n",
      "ic| len(x_data): 948\n",
      "ic| len(x_data): 960\n",
      "ic| len(x_data): 972\n",
      "ic| len(x_data): 984\n",
      "ic| len(x_data): 996\n",
      "ic| len(x_data): 1008\n",
      "ic| len(x_data): 1020\n",
      "ic| len(x_data): 1032\n",
      "ic| len(x_data): 1044\n",
      "ic| len(x_data): 1056\n",
      "ic| len(x_data): 1068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([854])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor     # for MSE or L1 Loss\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def create_datasets(csv_path: str, train_perc: float = 0.8):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    length = len(df)\n",
    "\n",
    "    x_data = np.array([])\n",
    "    y_data = np.array([])\n",
    "\n",
    "    # if x = curr angles and y = next angles\n",
    "    for i in range(length):\n",
    "        if i < length - 1:\n",
    "            x = np.array(df.iloc[i])\n",
    "            y = np.array(df.iloc[i + 1])\n",
    "        else:\n",
    "            #since it loops anyway\n",
    "            x = np.array(df.iloc[i])\n",
    "            y = np.array(df.iloc[0])\n",
    "        \n",
    "        x_data = np.append(x_data, x)\n",
    "        y_data = np.append(y_data,y)\n",
    "\n",
    "    # if x = timestamp and y = angles\n",
    "    # self.x_data = np.array(range(0, len(self.df)))\n",
    "    # for i in range(self.length):\n",
    "    #     self.y_data.append(self.df.iloc[i])\n",
    "    \n",
    "    # if x = both timestamp and curr_angles\n",
    "    # timestamps = range(0, len(self.df))\n",
    "    # for i in range(self.length):\n",
    "    #     if i < self.length - 1:\n",
    "    #         x = np.array(self.df.iloc[i])\n",
    "    #         y = np.array(self.df.iloc[i + 1])\n",
    "    #     else:\n",
    "    #         #since it loops anyway\n",
    "    #         x = np.array(self.df.iloc[i])\n",
    "    #         y = np.array(self.df.iloc[0])\n",
    "        \n",
    "    #     self.x_data.append((timestamps[i], x))\n",
    "    #     self.y_data.append(y)\n",
    "\n",
    "    \n",
    "    np.random.shuffle(x_data)\n",
    "    np.random.shuffle(y_data)\n",
    "\n",
    "    last_train_idx = int(len(x_data) * train_perc)\n",
    "\n",
    "    train_x = x_data[:last_train_idx]\n",
    "    train_y = y_data[:last_train_idx]\n",
    "    test_x = x_data[last_train_idx:]\n",
    "    test_y = y_data[last_train_idx:]\n",
    "\n",
    "    return AngleDataset(x=train_x, y=train_y), AngleDataset(x=test_x, y=test_y)\n",
    "\n",
    "train_dataset, test_dataset = create_datasets('./walk_angles.csv')\n",
    "\n",
    "train_dataset.x_data.shape\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training methods for the model\n",
    "These methods use an initialized model and training data to iteratively perform the forward and backward pass of optimization. Aside from some data reformatting that depends on the input, output, and loss function, these methods will always be the same for any shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, x, y, optimizer, loss_fn):\n",
    "    # Run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn, epochs=5):\n",
    "    losses = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for e in range(epochs):\n",
    "        for x, y in loader:\n",
    "            loss = train_batch(model=model, x=x, y=y, optimizer=optimizer, loss_fn=loss_fn)\n",
    "            losses.append(loss)\n",
    "\n",
    "            batch_index += 1\n",
    "\n",
    "        if e % 50 == 0:\n",
    "          ic(\"Epoch: \", e+1)\n",
    "          ic(\"Batches: \", batch_index)\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define testing methods for the model\n",
    "These methods are like training, but we don't need to update the parameters of the model anymore because when we call the test() method, the model has already been trained. Instead, this method just calculates the predicted y values and returns them, AKA the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, x, y):\n",
    "    # run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    return y, y_predict\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    y_vectors = list()\n",
    "    y_predict_vectors = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for x, y in loader:\n",
    "        y, y_predict = test_batch(model=model, x=x, y=y)\n",
    "\n",
    "        y_vectors.append(y.data.numpy())\n",
    "        y_predict_vectors.append(y_predict.data.numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "\n",
    "    y_predict_vector = np.concatenate(y_predict_vectors)\n",
    "\n",
    "    return y_predict_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting method for loss\n",
    "This is a plotting method for looking at the behavior of the loss over training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, show=True):\n",
    "    fig = pyplot.gcf()\n",
    "    fig.set_size_inches(8,6)\n",
    "    ax = pyplot.axes()\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    x_loss = list(range(len(losses)))\n",
    "    pyplot.plot(x_loss, losses)\n",
    "\n",
    "    if show:\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture\n",
    "- 12 inputs = 3 joint angles per leg, 4 legs\n",
    "- 12 outputs = *same as above*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchBrain(torch.nn.Module):\n",
    "    _id = 0 # Global genome identifier\n",
    "    _num_inputs = 12\n",
    "    _num_outputs = 12\n",
    "\n",
    "    # @classmethod\n",
    "    # def __get_new_id(cls):\n",
    "    #     cls._id += 1\n",
    "    #     return cls._id\n",
    "\n",
    "    @classmethod\n",
    "    def get_num_outputs(cls):\n",
    "        return cls._num_outputs\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.h1 = torch.nn.Linear(PytorchBrain._num_inputs, 12)\n",
    "        self.relu = torch.nn.ReLU() #output is same shape as input\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(12)\n",
    "        self.h2 = torch.nn.Linear(12, 12)\n",
    "        self.h3 = torch.nn.Linear(12, 12)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = torch.nn.Linear(12, PytorchBrain._num_outputs)\n",
    "        \n",
    "        # Define sigmoid activation\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.h1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.batch_norm(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = self.h2(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.batch_norm(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.batch_norm(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = self.tanh(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dataset, test_dataset, epochs=4):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 12\n",
    "    \n",
    "    data_loader_train = DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_test = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "    pytorch_model = PytorchBrain()\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(pytorch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss = ic(train(model=pytorch_model, loader=data_loader_train, optimizer=optimizer, loss_fn=loss_fn, epochs=epochs))\n",
    "    \n",
    "\n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = ic(test(model=pytorch_model, loader=data_loader_test))\n",
    "\n",
    "    return loss, y_predict, pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'Train set size: ', train_dataset.length: 854\n",
      "ic| 'Test set size: ', test_dataset.length: 214\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 2], m2: [12 x 12] at ..\\aten\\src\\TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17468\\1767946441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test set size: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final loss:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17468\\942793265.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(train_dataset, test_dataset, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Train and get the resulting loss per iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17468\\2525942660.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17468\\2525942660.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(model, x, y, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Run forward calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Compute loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17468\\762926135.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Pass the input tensor through each of our operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPytorchBrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# x = self.batch_norm(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\datascience\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 2], m2: [12 x 12] at ..\\aten\\src\\TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "\n",
    "ic(\"Train set size: \", train_dataset.length)\n",
    "ic(\"Test set size: \", test_dataset.length)\n",
    "\n",
    "losses, y_predict = run(train_dataset=train_dataset, test_dataset=test_dataset, epochs=400)\n",
    "\n",
    "ic(\"Final loss:\", sum(losses[-100:])/100)\n",
    "plot_loss(losses)\n",
    "\n",
    "# Transpose the matrices so we can plot.\n",
    "y_test = test_dataset.y_data.transpose()\n",
    "y_predict = y_predict.transpose()\n",
    "\n",
    "for yt, yp in zip(y_test, y_predict):\n",
    "  fig2 = pyplot.figure(dpi=300)\n",
    "  fig2.set_size_inches(8,6)\n",
    "  pyplot.scatter(test_dataset.x_data, yt, marker='o', s=0.2)\n",
    "  pyplot.scatter(test_dataset.x_data, yp, marker='o', s=0.3)\n",
    "  pyplot.text(-9, 0.44, \"- Prediction\", color=\"orange\", fontsize=8)\n",
    "  pyplot.text(-9, 0.48, \"- Sine (with noise)\", color=\"blue\", fontsize=8)\n",
    "  pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e96169c3d85a0b7abfdf8329564d63e23581ffb90e909b0554daf0689ed6e7b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
